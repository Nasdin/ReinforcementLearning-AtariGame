{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open AI Atari Games. Reinforcement Learning with PyTorch, deep Learning\n",
    "## By Nasrudin Bin Salim\n",
    "### Requirements: Python 2.7. Linux Environment/UNIX Environment\n",
    "    Please Install Pytorch\n",
    "    OpenAI Gym\n",
    "    Open AI Universe\n",
    "    cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import numpy as np\n",
    "\n",
    "import json\n",
    "import logging\n",
    "from cv2 import resize\n",
    "from skimage.color import rgb2gray\n",
    "import os\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\" #should be set to 1 to prevent conflicts\n",
    "import argparse\n",
    "import math\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "from torch.autograd import Variable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import OpenAI Universe environment and gym\n",
    "### Import Pytorch for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import universe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import pandas as pd\n",
    "from universe import vectorized\n",
    "from universe.wrappers import Unvectorize, Vectorize\n",
    "\n",
    "from gym.spaces.box import Box\n",
    "from gym.configuration import undo_logger_setup\n",
    "\n",
    "import torch\n",
    "from torch.multiprocessing import Process\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "#from skimage.transform import resize\n",
    "#from scipy.misc import imresize as resize\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions Section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logging function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def setup_logger(logger_name, log_file, level=logging.INFO):\n",
    "    l = logging.getLogger(logger_name)\n",
    "    formatter = logging.Formatter('%(asctime)s : %(message)s')\n",
    "    fileHandler = logging.FileHandler(log_file, mode='w')\n",
    "    fileHandler.setFormatter(formatter)\n",
    "    streamHandler = logging.StreamHandler()\n",
    "    streamHandler.setFormatter(formatter)\n",
    "\n",
    "    l.setLevel(level)\n",
    "    l.addHandler(fileHandler)\n",
    "    l.addHandler(streamHandler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Json Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_config(file_path):\n",
    "    \"\"\"Read JSON config.\"\"\"\n",
    "    json_object = json.load(open(file_path, 'r'))\n",
    "    return json_object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def norm_col_init(weights, std=1.0):\n",
    "    x = torch.randn(weights.size())\n",
    "    x *= std / torch.sqrt((x**2).sum(1, keepdim=True))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Share grads between 2 models\n",
    "#### More on this later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ensure_shared_grads(model, shared_model):\n",
    "    for param, shared_param in zip(model.parameters(),\n",
    "                                   shared_model.parameters()):\n",
    "        if shared_param.grad is not None:\n",
    "            return\n",
    "        shared_param._grad = param.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        weight_shape = list(m.weight.data.size())\n",
    "        fan_in = np.prod(weight_shape[1:4])\n",
    "        fan_out = np.prod(weight_shape[2:4]) * weight_shape[0]\n",
    "        w_bound = np.sqrt(6. / (fan_in + fan_out))\n",
    "        m.weight.data.uniform_(-w_bound, w_bound)\n",
    "        m.bias.data.fill_(0)\n",
    "    elif classname.find('Linear') != -1:\n",
    "        weight_shape = list(m.weight.data.size())\n",
    "        fan_in = weight_shape[1]\n",
    "        fan_out = weight_shape[0]\n",
    "        w_bound = np.sqrt(6. / (fan_in + fan_out))\n",
    "        m.weight.data.uniform_(-w_bound, w_bound)\n",
    "        m.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment, setting up the openAI and Universe "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the atari environment function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def atari_env(env_id, env_conf):\n",
    "    env = gym.make(env_id)\n",
    "    if len(env.observation_space.shape) > 1:\n",
    "        env = Vectorize(env)\n",
    "        env = AtariRescale(env, env_conf)\n",
    "        env = NormalizedEnv(env)\n",
    "        env = Unvectorize(env)\n",
    "        \n",
    "    return env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a frame for environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _process_frame(frame, conf):\n",
    "    frame = frame[conf[\"crop1\"]:conf[\"crop2\"] + 160, :160]\n",
    "    frame = resize(rgb2gray(frame), (80, conf[\"dimension2\"]))\n",
    "    frame = resize(frame, (80, 80))\n",
    "    frame = np.reshape(frame, [1, 80, 80])\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Atari rescale class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AtariRescale(vectorized.ObservationWrapper):\n",
    "    def __init__(self, env, env_conf):\n",
    "        super(AtariRescale, self).__init__(env)\n",
    "        self.observation_space = Box(0.0, 1.0, [1, 80, 80])\n",
    "        self.conf = env_conf\n",
    "\n",
    "    def _observation(self, observation_n):\n",
    "        return [\n",
    "            _process_frame(observation, self.conf)\n",
    "            for observation in observation_n\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalized environment class, where we can move from one state and observation to another"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NormalizedEnv(vectorized.ObservationWrapper):\n",
    "    def __init__(self, env=None):\n",
    "        super(NormalizedEnv, self).__init__(env)\n",
    "        self.state_mean = 0\n",
    "        self.state_std = 0\n",
    "        self.alpha = 0.9999\n",
    "        self.num_steps = 0\n",
    "\n",
    "    def _observation(self, observation_n):\n",
    "        for observation in observation_n:\n",
    "            self.num_steps += 1\n",
    "            self.state_mean = self.state_mean * self.alpha + \\\n",
    "                observation.mean() * (1 - self.alpha)\n",
    "            self.state_std = self.state_std * self.alpha + \\\n",
    "                observation.std() * (1 - self.alpha)\n",
    "\n",
    "        unbiased_mean = self.state_mean / (1 - pow(self.alpha, self.num_steps))\n",
    "        unbiased_std = self.state_std / (1 - pow(self.alpha, self.num_steps))\n",
    "\n",
    "        return [(observation - unbiased_mean) / (unbiased_std + 1e-8)\n",
    "                for observation in observation_n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "Using Google DeepMind's Idea. \n",
    "\n",
    "    Research Paper: https://arxiv.org/pdf/1602.01783.pdf\n",
    "    Asynchronous Advantage Actor-Critic (A3C)\n",
    "\n",
    "\n",
    "The A3C algorithm was released by Google’s DeepMind group earlier this year, and it made a splash by… essentially obsoleting DQN. It was faster, simpler, more robust, and able to achieve much better scores on the standard battery of Deep RL tasks. On top of all that it could work in continuous as well as discrete action spaces. Given this, it has become the go-to Deep RL algorithm for new challenging problems with complex state and action spaces\n",
    "\n",
    "\n",
    "    \n",
    "<a href= \"https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-8-asynchronous-actor-critic-agents-a3c-c88f72a5e9f2\" >Medium Article explaining A3c reinforcement learning </a>\n",
    "\n",
    "## The Actor-Critic Structure\n",
    "<img src = \"img/A3CStructure.png\">\n",
    "\n",
    "## Many workers training and learning concurrently, and then updates global network with gradients\n",
    "### Process Flow\n",
    "<img src = \"img/A3CProcessFlow.png\">\n",
    "    \n",
    "### Long Short Term Memory Recurrent Neural Nets\n",
    "    \n",
    "## Implementing in Pytorch, with a class and on LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class A3Clstm(torch.nn.Module):\n",
    "    def __init__(self, num_inputs, action_space):\n",
    "        super(A3Clstm, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(num_inputs, 32, 5, stride=1, padding=2)\n",
    "        self.maxp1 = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(32, 32, 5, stride=1, padding=1)\n",
    "        self.maxp2 = nn.MaxPool2d(2, 2)\n",
    "        self.conv3 = nn.Conv2d(32, 64, 4, stride=1, padding=1)\n",
    "        self.maxp3 = nn.MaxPool2d(2, 2)\n",
    "        self.conv4 = nn.Conv2d(64, 64, 3, stride=1, padding=1)\n",
    "        self.maxp4 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        self.lstm = nn.LSTMCell(1024, 512)\n",
    "        num_outputs = action_space.n\n",
    "        self.critic_linear = nn.Linear(512, 1)\n",
    "        self.actor_linear = nn.Linear(512, num_outputs)\n",
    "\n",
    "        self.apply(weights_init)\n",
    "        self.actor_linear.weight.data = norm_col_init(\n",
    "            self.actor_linear.weight.data, 0.01)\n",
    "        self.actor_linear.bias.data.fill_(0)\n",
    "        self.critic_linear.weight.data = norm_col_init(\n",
    "            self.critic_linear.weight.data, 1.0)\n",
    "        self.critic_linear.bias.data.fill_(0)\n",
    "\n",
    "        self.lstm.bias_ih.data.fill_(0)\n",
    "        self.lstm.bias_hh.data.fill_(0)\n",
    "\n",
    "        self.train()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        inputs, (hx, cx) = inputs\n",
    "        x = F.relu(self.maxp1(self.conv1(inputs)))\n",
    "        x = F.relu(self.maxp2(self.conv2(x)))\n",
    "        x = F.relu(self.maxp3(self.conv3(x)))\n",
    "        x = F.relu(self.maxp4(self.conv4(x)))\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        hx, cx = self.lstm(x, (hx, cx))\n",
    "\n",
    "        x = hx\n",
    "\n",
    "        return self.critic_linear(x), self.actor_linear(x), (hx, cx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The player Agent\n",
    "## (Reinforcement Learning agent to interact with the env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    def __init__(self, model, env, args, state):\n",
    "        self.model = model\n",
    "        self.env = env\n",
    "        self.current_life = 0\n",
    "        self.state = state\n",
    "        self.hx = None\n",
    "        self.cx = None\n",
    "        self.eps_len = 0\n",
    "        self.args = args\n",
    "        self.values = []\n",
    "        self.log_probs = []\n",
    "        self.rewards = []\n",
    "        self.entropies = []\n",
    "        self.done = True\n",
    "        self.info = None\n",
    "        self.reward = 0\n",
    "\n",
    "    def action_train(self):\n",
    "        if self.done:\n",
    "            self.cx = Variable(torch.zeros(1, 512))\n",
    "            self.hx = Variable(torch.zeros(1, 512))\n",
    "        else:\n",
    "            self.cx = Variable(self.cx.data)\n",
    "            self.hx = Variable(self.hx.data)\n",
    "        value, logit, (self.hx, self.cx) = self.model((Variable(self.state.unsqueeze(0)), (self.hx, self.cx)))\n",
    "        prob = F.softmax(logit)\n",
    "        log_prob = F.log_softmax(logit)\n",
    "        entropy = -(log_prob * prob).sum(1)\n",
    "        self.entropies.append(entropy)\n",
    "        action = prob.multinomial().data\n",
    "        log_prob = log_prob.gather(1, Variable(action))\n",
    "        state, self.reward, self.done, self.info = self.env.step(action.numpy())\n",
    "        self.state = torch.from_numpy(state).float()\n",
    "        self.eps_len += 1\n",
    "        self.done = self.done or self.eps_len >= self.args['M']\n",
    "        self.reward = max(min(self.reward, 1), -1)\n",
    "        self.values.append(value)\n",
    "        self.log_probs.append(log_prob)\n",
    "        self.rewards.append(self.reward)\n",
    "        return self\n",
    "\n",
    "    def action_test(self):\n",
    "        if self.done:\n",
    "            self.cx = Variable(torch.zeros(1, 512), volatile=True)\n",
    "            self.hx = Variable(torch.zeros(1, 512), volatile=True)\n",
    "        else:\n",
    "            self.cx = Variable(self.cx.data, volatile=True)\n",
    "            self.hx = Variable(self.hx.data, volatile=True)\n",
    "        value, logit, (self.hx, self.cx) = self.model((Variable(self.state.unsqueeze(0), volatile=True), (self.hx, self.cx)))\n",
    "        prob = F.softmax(logit)\n",
    "        action = prob.max(1)[1].data.numpy()\n",
    "        state, self.reward, self.done, self.info = self.env.step(action[0])\n",
    "        self.state = torch.from_numpy(state).float()\n",
    "        self.eps_len += 1\n",
    "        self.done = self.done or self.eps_len >= self.args['M']\n",
    "        return self\n",
    "\n",
    "    def check_state(self):\n",
    "        if self.current_life > self.info['ale.lives']:\n",
    "            self.done = True\n",
    "        self.current_life = self.info['ale.lives']\n",
    "        return self\n",
    "\n",
    "    def clear_actions(self):\n",
    "        self.values = []\n",
    "        self.log_probs = []\n",
    "        self.rewards = []\n",
    "        self.entropies = []\n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shared Memory and optimization algorithims\n",
    "## As Part of the A3C Network, multiple workers will be working together to update a global network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSprop\n",
    "\n",
    "RMSprop is an unpublished, adaptive learning rate method proposed by Geoff Hinton in Lecture 6e of his Coursera Class.\n",
    "\n",
    "RMSprop and Adadelta have both been developed independently around the same time stemming from the need to resolve Adagrad's radically diminishing learning rates. RMSprop in fact is identical to the first update vector of Adadelta \n",
    "\n",
    "RMSprop as well divides the learning rate by an exponentially decaying average of squared gradients. Hinton suggests γ\n",
    "to be set to 0.9, while a good default value for the learning rate η is 0.001."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SharedRMSprop(optim.RMSprop):\n",
    "    \"\"\"Implements RMSprop algorithm with shared states.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 params,\n",
    "                 lr=7e-4,\n",
    "                 alpha=0.99,\n",
    "                 eps=0.1,\n",
    "                 weight_decay=0,\n",
    "                 momentum=0,\n",
    "                 centered=False):\n",
    "        super(SharedRMSprop, self).__init__(params, lr, alpha, eps,\n",
    "                                            weight_decay, momentum, centered)\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                state = self.state[p]\n",
    "                state['step'] = torch.zeros(1)\n",
    "                state['grad_avg'] = p.data.new().resize_as_(p.data).zero_()\n",
    "                state['square_avg'] = p.data.new().resize_as_(p.data).zero_()\n",
    "                state['momentum_buffer'] = p.data.new().resize_as_(\n",
    "                    p.data).zero_()\n",
    "\n",
    "    def share_memory(self):\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                state = self.state[p]\n",
    "                state['square_avg'].share_memory_()\n",
    "                state['step'].share_memory_()\n",
    "                state['grad_avg'].share_memory_()\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"Performs a single optimization step.\n",
    "        Arguments:\n",
    "            closure (callable, optional): A closure that reevaluates the model\n",
    "                and returns the loss.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data\n",
    "                state = self.state[p]\n",
    "\n",
    "                square_avg = state['square_avg']\n",
    "                alpha = group['alpha']\n",
    "\n",
    "                state['step'] += 1\n",
    "\n",
    "                if group['weight_decay'] != 0:\n",
    "                    grad = grad.add(group['weight_decay'], p.data)\n",
    "\n",
    "                square_avg.mul_(alpha).addcmul_(1 - alpha, grad, grad)\n",
    "\n",
    "                if group['centered']:\n",
    "                    grad_avg = state['grad_avg']\n",
    "                    grad_avg.mul_(alpha).add_(1 - alpha, grad)\n",
    "                    avg = square_avg.addcmul(\n",
    "                        -1, grad_avg, grad_avg).sqrt().add_(group['eps'])\n",
    "                else:\n",
    "                    avg = square_avg.sqrt().add_(group['eps'])\n",
    "\n",
    "                if group['momentum'] > 0:\n",
    "                    buf = state['momentum_buffer']\n",
    "                    buf.mul_(group['momentum']).addcdiv_(grad, avg)\n",
    "                    p.data.add_(-group['lr'], buf)\n",
    "                else:\n",
    "                    p.data.addcdiv_(-group['lr'], grad, avg)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaptive Moment Estimation (Adam) \n",
    "is another method that computes adaptive learning rates for each parameter. In addition to storing an exponentially decaying average of past squared gradients vt like Adadelta and RMSprop, Adam also keeps an exponentially decaying average of past gradients mt, similar to momentum:\n",
    "\n",
    "Adam (short for Adaptive Moment Estimation) is an update to the RMSProp optimizer. In this optimization algorithm, running averages of both the gradients and the second moments of the gradients are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SharedAdam(optim.Adam):\n",
    "    \"\"\"Implements Adam algorithm with shared states.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 params,\n",
    "                 lr=1e-3,\n",
    "                 betas=(0.9, 0.999),\n",
    "                 eps=1e-3,\n",
    "                 weight_decay=0):\n",
    "        super(SharedAdam, self).__init__(params, lr, betas, eps, weight_decay)\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                state = self.state[p]\n",
    "                state['step'] = torch.zeros(1)\n",
    "                state['exp_avg'] = p.data.new().resize_as_(p.data).zero_()\n",
    "                state['exp_avg_sq'] = p.data.new().resize_as_(p.data).zero_()\n",
    "\n",
    "    def share_memory(self):\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                state = self.state[p]\n",
    "                state['step'].share_memory_()\n",
    "                state['exp_avg'].share_memory_()\n",
    "                state['exp_avg_sq'].share_memory_()\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"Performs a single optimization step.\n",
    "        Arguments:\n",
    "            closure (callable, optional): A closure that reevaluates the model\n",
    "                and returns the loss.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data\n",
    "                state = self.state[p]\n",
    "\n",
    "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "                beta1, beta2 = group['betas']\n",
    "\n",
    "                state['step'] += 1\n",
    "\n",
    "                if group['weight_decay'] != 0:\n",
    "                    grad = grad.add(group['weight_decay'], p.data)\n",
    "\n",
    "                # Decay the first and second moment running average coefficient\n",
    "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
    "\n",
    "                denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
    "\n",
    "                bias_correction1 = 1 - beta1**state['step'][0]\n",
    "                bias_correction2 = 1 - beta2**state['step'][0]\n",
    "                step_size = group['lr'] * \\\n",
    "                    math.sqrt(bias_correction2) / bias_correction1\n",
    "\n",
    "                p.data.addcdiv_(-step_size, exp_avg, denom)\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "sample_lr = [\n",
    "    0.0001, 0.00009, 0.00008, 0.00007, 0.00006, 0.00005, 0.00004, 0.00003,\n",
    "    0.00002, 0.00001, 0.000009, 0.000008, 0.000007, 0.000006, 0.000005,\n",
    "    0.000004, 0.000003, 0.000002, 0.000001\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam but only with shared Lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SharedLrSchedAdam(optim.Adam):\n",
    "    \"\"\"Implements Adam algorithm with shared states.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 params,\n",
    "                 lr=1e-3,\n",
    "                 betas=(0.9, 0.999),\n",
    "                 eps=1e-3,\n",
    "                 weight_decay=0):\n",
    "        super(SharedLrSchedAdam, self).__init__(params, lr, betas, eps,\n",
    "                                                weight_decay)\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                state = self.state[p]\n",
    "                state['step'] = torch.zeros(1)\n",
    "                state['exp_avg'] = p.data.new().resize_as_(p.data).zero_()\n",
    "                state['exp_avg_sq'] = p.data.new().resize_as_(p.data).zero_()\n",
    "\n",
    "    def share_memory(self):\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                state = self.state[p]\n",
    "                state['step'].share_memory_()\n",
    "                state['exp_avg'].share_memory_()\n",
    "                state['exp_avg_sq'].share_memory_()\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"Performs a single optimization step.\n",
    "        Arguments:\n",
    "            closure (callable, optional): A closure that reevaluates the model\n",
    "                and returns the loss.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        lr = sample_lr[int(state['step'][0] // 40000000)]\n",
    "        group['lr'] = lr\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data\n",
    "                state = self.state[p]\n",
    "\n",
    "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "                beta1, beta2 = group['betas']\n",
    "\n",
    "                state['step'] += 1\n",
    "\n",
    "                if group['weight_decay'] != 0:\n",
    "                    grad = grad.add(group['weight_decay'], p.data)\n",
    "\n",
    "                # Decay the first and second moment running average coefficient\n",
    "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
    "\n",
    "                denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
    "\n",
    "                bias_correction1 = 1 - beta1**state['step'][0]\n",
    "                bias_correction2 = 1 - beta2**state['step'][0]\n",
    "                step_size = group['lr'] * \\\n",
    "                    math.sqrt(bias_correction2) / bias_correction1\n",
    "                p.data.addcdiv_(-step_size, exp_avg, denom)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions to run the model on the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test\n",
    "## Function To test the model on a game/environ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test(args, shared_model, env_conf,render=False):\n",
    "    log = {}\n",
    "    setup_logger('{}_log'.format(args['ENV']),\n",
    "                 r'{0}{1}_log'.format(args['LG'], args['ENV']))\n",
    "    log['{}_log'.format(args['ENV'])] = logging.getLogger(\n",
    "        '{}_log'.format(args['ENV']))\n",
    "    d_args = args\n",
    "    for k in d_args.keys():\n",
    "        log['{}_log'.format(args['ENV'])].info('{0}: {1}'.format(k, d_args[k]))\n",
    "\n",
    "    torch.manual_seed(args['seed'])\n",
    "    env = atari_env(args['ENV'], env_conf)\n",
    "    reward_sum = 0\n",
    "    start_time = time.time()\n",
    "    num_tests = 0\n",
    "    reward_total_sum = 0\n",
    "    player = Agent(None, env, args, None)\n",
    "    player.model = A3Clstm(\n",
    "        player.env.observation_space.shape[0], player.env.action_space)\n",
    "    player.state = player.env.reset()\n",
    "    player.state = torch.from_numpy(player.state).float()\n",
    "    player.model.eval()\n",
    "\n",
    "    while True:\n",
    "        if player.done:\n",
    "            player.model.load_state_dict(shared_model.state_dict())\n",
    "        if render:\n",
    "            env.render()\n",
    "        player.action_test()\n",
    "        reward_sum += player.reward\n",
    "\n",
    "        if player.done:\n",
    "            num_tests += 1\n",
    "            player.current_life = 0\n",
    "            reward_total_sum += reward_sum\n",
    "            reward_mean = reward_total_sum / num_tests\n",
    "            log['{}_log'.format(args['ENV'])].info(\n",
    "                \"Time {0}, episode reward {1}, episode length {2}, reward mean {3:.4f}\".\n",
    "                format(\n",
    "                    time.strftime(\"%Hh %Mm %Ss\",\n",
    "                                  time.gmtime(time.time() - start_time)),\n",
    "                    reward_sum, player.eps_len, reward_mean))\n",
    "\n",
    "            if reward_sum > args['SSL']:\n",
    "                player.model.load_state_dict(shared_model.state_dict())\n",
    "                state_to_save = player.model.state_dict()\n",
    "                torch.save(state_to_save, '{0}{1}.dat'.format(\n",
    "                    args['SMD'], args['ENV']))\n",
    "\n",
    "            reward_sum = 0\n",
    "            player.eps_len = 0\n",
    "            state = player.env.reset()\n",
    "            time.sleep(60)\n",
    "            player.state = torch.from_numpy(state).float()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train\n",
    "## Function to Train the model with an optimizer algorithim on an environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(rank, args, shared_model, optimizer, env_conf):\n",
    "\n",
    "    torch.manual_seed(args['seed'] + rank)\n",
    "    env = atari_env(args['ENV'], env_conf)\n",
    "    if optimizer is None:\n",
    "        if args['OPT'] == 'RMSprop':\n",
    "            optimizer = optim.RMSprop(shared_model.parameters(), lr=args['LR'])\n",
    "        if args['OPT'] == 'Adam':\n",
    "            optimizer = optim.Adam(shared_model.parameters(), lr=args['LR'])\n",
    "\n",
    "    env.seed(args['seed'] + rank)\n",
    "    player = Agent(None, env, args, None)\n",
    "    player.model = A3Clstm(\n",
    "        player.env.observation_space.shape[0], player.env.action_space)\n",
    "    player.state = player.env.reset()\n",
    "    player.state = torch.from_numpy(player.state).float()\n",
    "    player.model.train()\n",
    "\n",
    "    while True:\n",
    "        player.model.load_state_dict(shared_model.state_dict())\n",
    "        for step in range(args['NS']):\n",
    "            player.action_train()\n",
    "            if args['CL']:\n",
    "                player.check_state()\n",
    "            if player.done:\n",
    "                break\n",
    "\n",
    "        if player.done:\n",
    "            player.eps_len = 0\n",
    "            player.current_life = 0\n",
    "            state = player.env.reset()\n",
    "            player.state = torch.from_numpy(state).float()\n",
    "\n",
    "        R = torch.zeros(1, 1)\n",
    "        if not player.done:\n",
    "            value, _, _ = player.model(\n",
    "                (Variable(player.state.unsqueeze(0)), (player.hx, player.cx)))\n",
    "            R = value.data\n",
    "\n",
    "        player.values.append(Variable(R))\n",
    "        policy_loss = 0\n",
    "        value_loss = 0\n",
    "        R = Variable(R)\n",
    "        gae = torch.zeros(1, 1)\n",
    "        for i in reversed(range(len(player.rewards))):\n",
    "            R = args['G'] * R + player.rewards[i]\n",
    "            advantage = R - player.values[i]\n",
    "            value_loss = value_loss + 0.5 * advantage.pow(2)\n",
    "\n",
    "            # Generalized Advantage Estimataion\n",
    "            delta_t = player.rewards[i] + args['G'] * \\\n",
    "                player.values[i + 1].data - player.values[i].data\n",
    "            gae = gae * args['G'] * args['T'] + delta_t\n",
    "\n",
    "            policy_loss = policy_loss - \\\n",
    "                player.log_probs[i] * \\\n",
    "                Variable(gae) - 0.01 * player.entropies[i]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        (policy_loss + 0.5 * value_loss).backward()\n",
    "        torch.nn.utils.clip_grad_norm(player.model.parameters(), 40)\n",
    "        ensure_shared_grads(player.model, shared_model)\n",
    "        optimizer.step()\n",
    "        player.clear_actions()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting it altogether"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Desription"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Parameter: LR\n",
    "    Type: float\n",
    "    Description: Learning Rate\n",
    "##### Parameter: G\n",
    "    Type=float,\n",
    "    Description: discount factor for rewards (default: 0.99)\n",
    "##### Parameter: T\n",
    "    Type=float,\n",
    "    Description: parameter for GAE (default: 1.00)\n",
    "##### Parameter:seed\n",
    "    Type: int\n",
    "    Descrition: random seed (default: 42)\n",
    "##### Parameter:W\n",
    "    Type=int,\n",
    "    Description: how many training processes to use (default: 5)\n",
    "##### Parameter: NS\n",
    "    Type=int,\n",
    "    Description: number of forward steps in A3C (default: 20)\n",
    "##### Parameter: M\n",
    "    Type=int,\n",
    "    Description: maximum length of an episode (default: 10000)\n",
    "##### Parameter: ENV\n",
    "    Description: environment to train on (default: Pong-v0)\n",
    "##### Parameter: EC\n",
    "    Description: environment to crop and resize info (default: config.json)\n",
    "##### Parameter: SO\n",
    "    Description: use an optimizer without shared statistics.(default: True)\n",
    "##### Parameter: L\n",
    "    Description: load a trained model, (default: False)\n",
    "##### Parameter: SSL\n",
    "    Type=int,\n",
    "    Description: reward score test evaluation must get higher than to save model (default:20)\n",
    "##### Parameter: OPT\n",
    "    Description: shares optimizer choice of Adam, LrSchedAdam or RMSprop (default: Adam)\n",
    "##### Parameter: CL\n",
    "    Description: end of life is end of training episode.(default: False)\n",
    "##### Parameter: LMD\n",
    "    Description: folder to load trained models from (default: 'trained_models/')\n",
    "##### Parameter: SMD\n",
    "    Description: folder to save trained models (default: '/trained_models/')\n",
    "##### Parameter: LG\n",
    "    Description: folder to save logs (default: '/logs/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List of Games, pick one here and then edit the environment accordingly\n",
    "    Choose an Atari game and it has to be a 4D Tensor Game \n",
    "    Or if you don't know what that means, just guess and check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[EnvSpec(flashgames.UrbanMicroRacers-v0),\n",
       " EnvSpec(flashgames.PlopPlopLite-v0),\n",
       " EnvSpec(DoubleDunk-ramDeterministic-v4),\n",
       " EnvSpec(flashgames.Sieger2LevelPack-v0),\n",
       " EnvSpec(DoubleDunk-ramDeterministic-v0),\n",
       " EnvSpec(gym-core.Krull-v0),\n",
       " EnvSpec(gym-core.Krull-v3),\n",
       " EnvSpec(Pooyan-ram-v4),\n",
       " EnvSpec(Pooyan-ram-v0),\n",
       " EnvSpec(flashgames.GonAndMon-v0),\n",
       " EnvSpec(flashgames.NeonRaceLvl4-v0),\n",
       " EnvSpec(flashgames.Hash-v0),\n",
       " EnvSpec(gym-core.JourneyEscapeSlow-v3),\n",
       " EnvSpec(gym-core.JourneyEscapeSlow-v0),\n",
       " EnvSpec(flashgames.FlashBombs-v0),\n",
       " EnvSpec(gym-core.JamesbondDeterministicSlow-v0),\n",
       " EnvSpec(VentureNoFrameskip-v0),\n",
       " EnvSpec(Centipede-v0),\n",
       " EnvSpec(Centipede-v4),\n",
       " EnvSpec(flashgames.Crumbs2-v0),\n",
       " EnvSpec(flashgames.CosmoGravity2-v0),\n",
       " EnvSpec(gym-core.Zaxxon30FPS-v0),\n",
       " EnvSpec(gym-core.Zaxxon30FPS-v3),\n",
       " EnvSpec(flashgames.ZombiesAndDonuts-v0),\n",
       " EnvSpec(Frostbite-ramNoFrameskip-v0),\n",
       " EnvSpec(Frostbite-ramNoFrameskip-v4),\n",
       " EnvSpec(IceHockey-ramNoFrameskip-v4),\n",
       " EnvSpec(flashgames.SpacePunkRacerLvl8-v0),\n",
       " EnvSpec(flashgames.EvolutionRacingLvl8-v0),\n",
       " EnvSpec(flashgames.GalacticGems2NewFrontiers-v0),\n",
       " EnvSpec(flashgames.DisasterWillStrikeDefender-v0),\n",
       " EnvSpec(flashgames.BikeTrial2-v0),\n",
       " EnvSpec(gym-core.SolarisDeterministic-v3),\n",
       " EnvSpec(gym-core.SolarisDeterministic-v0),\n",
       " EnvSpec(AirRaidNoFrameskip-v4),\n",
       " EnvSpec(gym-core.BerzerkSlow-v3),\n",
       " EnvSpec(AirRaidNoFrameskip-v0),\n",
       " EnvSpec(gym-core.BankHeistSync-v3),\n",
       " EnvSpec(flashgames.NinjaTrainingWorlds-v0),\n",
       " EnvSpec(flashgames.3dFlashRacer-v0),\n",
       " EnvSpec(KrullDeterministic-v0),\n",
       " EnvSpec(SemisuperPendulumRandom-v0),\n",
       " EnvSpec(KrullDeterministic-v4),\n",
       " EnvSpec(wob.real.Quizlet-Planet-Test-v0),\n",
       " EnvSpec(Go19x19-v0),\n",
       " EnvSpec(gym-core.CrazyClimber30FPS-v0),\n",
       " EnvSpec(flashgames.GravityBall-v0),\n",
       " EnvSpec(gym-core.CrazyClimber30FPS-v3),\n",
       " EnvSpec(gym-core.ChopperCommandDeterministicSlow-v0),\n",
       " EnvSpec(gym-core.ChopperCommandDeterministicSlow-v3),\n",
       " EnvSpec(gym-core.CentipedeDeterministicSlow-v0),\n",
       " EnvSpec(gym-core.Zaxxon-v0),\n",
       " EnvSpec(gym-core.CentipedeDeterministicSlow-v3),\n",
       " EnvSpec(FishingDerbyNoFrameskip-v4),\n",
       " EnvSpec(FishingDerbyNoFrameskip-v0),\n",
       " EnvSpec(flashgames.FormulaXspeed3d-v0),\n",
       " EnvSpec(flashgames.BobbyNutcaseMotoJumping-v0),\n",
       " EnvSpec(flashgames.RollerRider-v0),\n",
       " EnvSpec(gym-core.YarsRevengeNoFrameskip-v0),\n",
       " EnvSpec(gym-core.Assault30FPS-v0),\n",
       " EnvSpec(gym-core.Assault30FPS-v3),\n",
       " EnvSpec(Qbert-v4),\n",
       " EnvSpec(Qbert-v0),\n",
       " EnvSpec(flashgames.BusinessmanSimulator-v0),\n",
       " EnvSpec(gym-core.MontezumaRevenge30FPS-v3),\n",
       " EnvSpec(flashgames.PunchBallJump-v0),\n",
       " EnvSpec(Robotank-ram-v4),\n",
       " EnvSpec(flashgames.SpectrumRunner-v0),\n",
       " EnvSpec(flashgames.LooneyAndJohny-v0),\n",
       " EnvSpec(gym-core.TennisDeterministicSync-v3),\n",
       " EnvSpec(flashgames.TheThreeTowers-v0),\n",
       " EnvSpec(gym-core.TennisDeterministicSync-v0),\n",
       " EnvSpec(flashgames.DriveToWreck-v0),\n",
       " EnvSpec(flashgames.SkiSim-v0),\n",
       " EnvSpec(MontezumaRevenge-ramNoFrameskip-v4),\n",
       " EnvSpec(MontezumaRevenge-ramNoFrameskip-v0),\n",
       " EnvSpec(gym-core.BerzerkDeterministicSync-v0),\n",
       " EnvSpec(flashgames.FlashRacer-v0),\n",
       " EnvSpec(flashgames.Sundrops-v0),\n",
       " EnvSpec(gym-core.TimePilot30FPS-v0),\n",
       " EnvSpec(gym-core.TimePilot30FPS-v3),\n",
       " EnvSpec(flashgames.Rocketeer-v0),\n",
       " EnvSpec(Go9x9-v0),\n",
       " EnvSpec(flashgames.MineDrop-v0),\n",
       " EnvSpec(flashgames.RollingHills-v0),\n",
       " EnvSpec(flashgames.DrawGems-v0),\n",
       " EnvSpec(MountainCarContinuous-v0),\n",
       " EnvSpec(gym-core.CrazyClimberDeterministicSync-v3),\n",
       " EnvSpec(gym-core.CrazyClimberDeterministicSync-v0),\n",
       " EnvSpec(gym-core.EnduroSync-v0),\n",
       " EnvSpec(flashgames.WastelandSiege-v0),\n",
       " EnvSpec(gym-core.EnduroSync-v3),\n",
       " EnvSpec(gym-core.Zaxxon-v3),\n",
       " EnvSpec(Pong-v4),\n",
       " EnvSpec(flashgames.PapaLouie3WhenSundaesAttack-v0),\n",
       " EnvSpec(Pong-v0),\n",
       " EnvSpec(flashgames.StormRage-v0),\n",
       " EnvSpec(flashgames.GalleonFight-v0),\n",
       " EnvSpec(gym-core.BattleZone30FPS-v0),\n",
       " EnvSpec(gym-core.BattleZone30FPS-v3),\n",
       " EnvSpec(flashgames.GalacticGems2LevelPack-v0),\n",
       " EnvSpec(flashgames.MagicSafari-v0),\n",
       " EnvSpec(gym-core.CentipedeDeterministicSync-v3),\n",
       " EnvSpec(gym-core.CentipedeDeterministicSync-v0),\n",
       " EnvSpec(gym-core.UpNDownSync-v0),\n",
       " EnvSpec(gym-core.UpNDownSync-v3),\n",
       " EnvSpec(flashgames.RhythmSnake-v0),\n",
       " EnvSpec(flashgames.SuperK9-v0),\n",
       " EnvSpec(gym-core.KrullNoFrameskip-v3),\n",
       " EnvSpec(flashgames.AmericanRacingLvl2-v0),\n",
       " EnvSpec(flashgames.GroundBattles-v0),\n",
       " EnvSpec(BattleZoneDeterministic-v4),\n",
       " EnvSpec(BattleZoneDeterministic-v0),\n",
       " EnvSpec(gym-core.Asterix-v0),\n",
       " EnvSpec(gym-core.Asterix-v3),\n",
       " EnvSpec(flashgames.AmericanRacingLvl19-v0),\n",
       " EnvSpec(flashgames.MysteriousPirateJewels-v0),\n",
       " EnvSpec(flashgames.GemPop-v0),\n",
       " EnvSpec(gym-core.AirRaid-v3),\n",
       " EnvSpec(flashgames.JumpOverTheRings-v0),\n",
       " EnvSpec(gym-core.SeaquestDeterministic-v3),\n",
       " EnvSpec(gym-core.SeaquestDeterministic-v0),\n",
       " EnvSpec(ConvergenceControl-v0),\n",
       " EnvSpec(flashgames.MonkeyManic-v0),\n",
       " EnvSpec(flashgames.SurvivalLab-v0),\n",
       " EnvSpec(flashgames.MeerkatMission-v0),\n",
       " EnvSpec(gym-core.AlienSync-v0),\n",
       " EnvSpec(gym-core.AlienSync-v3),\n",
       " EnvSpec(gym-core.SkiingSlow-v3),\n",
       " EnvSpec(gym-core.SkiingSlow-v0),\n",
       " EnvSpec(BattleZoneNoFrameskip-v0),\n",
       " EnvSpec(gym-core.SpaceInvaders30FPS-v3),\n",
       " EnvSpec(flashgames.BulletHeaven-v0),\n",
       " EnvSpec(BattleZoneNoFrameskip-v4),\n",
       " EnvSpec(gym-core.DemonAttackNoFrameskip-v3),\n",
       " EnvSpec(gym-core.DemonAttackNoFrameskip-v0),\n",
       " EnvSpec(flashgames.MexicoRex-v0),\n",
       " EnvSpec(gym-core.CentipedeSlow-v0),\n",
       " EnvSpec(gym-core.CentipedeSlow-v3),\n",
       " EnvSpec(flashgames.Pyro-v0),\n",
       " EnvSpec(TutankhamDeterministic-v0),\n",
       " EnvSpec(UpNDown-ramDeterministic-v4),\n",
       " EnvSpec(TutankhamDeterministic-v4),\n",
       " EnvSpec(UpNDown-ramDeterministic-v0),\n",
       " EnvSpec(gym-core.AirRaidDeterministic-v0),\n",
       " EnvSpec(gym-core.AirRaidDeterministic-v3),\n",
       " EnvSpec(flashgames.ShimmyChute-v0),\n",
       " EnvSpec(Jamesbond-ramNoFrameskip-v4),\n",
       " EnvSpec(flashgames.SuperbikeRacer-v0),\n",
       " EnvSpec(Jamesbond-ramNoFrameskip-v0),\n",
       " EnvSpec(gym-core.DoubleDunkDeterministic-v3),\n",
       " EnvSpec(gym-core.DoubleDunkDeterministic-v0),\n",
       " EnvSpec(flashgames.Krome-v0),\n",
       " EnvSpec(gym-core.KrullSlow-v0),\n",
       " EnvSpec(gym-core.KrullSlow-v3),\n",
       " EnvSpec(wob.mini.ScrollText2-v0),\n",
       " EnvSpec(gym-core.IceHockey-v3),\n",
       " EnvSpec(gym-core.IceHockey-v0),\n",
       " EnvSpec(gym-core.AsterixDeterministic-v3),\n",
       " EnvSpec(gym-core.AsterixDeterministic-v0),\n",
       " EnvSpec(flashgames.KeeperOfTheGrove3-v0),\n",
       " EnvSpec(YarsRevengeDeterministic-v0),\n",
       " EnvSpec(YarsRevengeDeterministic-v4),\n",
       " EnvSpec(gym-core.StarGunnerDeterministicSlow-v3),\n",
       " EnvSpec(gym-core.Tutankham-v3),\n",
       " EnvSpec(gym-core.Tutankham-v0),\n",
       " EnvSpec(flashgames.CatchTheStar-v0),\n",
       " EnvSpec(Bowling-ramNoFrameskip-v0),\n",
       " EnvSpec(Bowling-ramNoFrameskip-v4),\n",
       " EnvSpec(TwoRoundNondeterministicReward-v0),\n",
       " EnvSpec(flashgames.FlashRace-v0),\n",
       " EnvSpec(flashgames.TattooArtist-v0),\n",
       " EnvSpec(flashgames.FormulaRacer2012Lvl8-v0),\n",
       " EnvSpec(flashgames.DumperRush-v0),\n",
       " EnvSpec(flashgames.LaserCannon3LevelsPack-v0),\n",
       " EnvSpec(flashgames.SlipSlideSloth-v0),\n",
       " EnvSpec(flashgames.PaulVaulting-v0),\n",
       " EnvSpec(flashgames.CoasterRacer2Lvl2-v0),\n",
       " EnvSpec(flashgames.DigToChina-v0),\n",
       " EnvSpec(gym-core.Bowling-v0),\n",
       " EnvSpec(flashgames.ZombieTdReborn-v0),\n",
       " EnvSpec(CrazyClimber-ram-v0),\n",
       " EnvSpec(gym-core.KungFuMaster30FPS-v0),\n",
       " EnvSpec(CrazyClimber-ram-v4),\n",
       " EnvSpec(flashgames.WoollyBearJigsawPuzzle-v0),\n",
       " EnvSpec(gym-core.Pooyan30FPS-v3),\n",
       " EnvSpec(RoadRunner-ramNoFrameskip-v4),\n",
       " EnvSpec(RoadRunner-ramNoFrameskip-v0),\n",
       " EnvSpec(flashgames.SuperRallyExtreme-v0),\n",
       " EnvSpec(Jamesbond-ramDeterministic-v4),\n",
       " EnvSpec(gym-core.Pooyan-v3),\n",
       " EnvSpec(BreakoutDeterministic-v4),\n",
       " EnvSpec(gym-core.Pooyan30FPS-v0),\n",
       " EnvSpec(Jamesbond-ramDeterministic-v0),\n",
       " EnvSpec(flashgames.AmericanRacingLvl14-v0),\n",
       " EnvSpec(BreakoutDeterministic-v0),\n",
       " EnvSpec(flashgames.CoasterRacer2Lvl3-v0),\n",
       " EnvSpec(flashgames.HeatRushFutureLvl14-v0),\n",
       " EnvSpec(flashgames.HeatRushFutureLvl15-v0),\n",
       " EnvSpec(flashgames.BottleCaps-v0),\n",
       " EnvSpec(gym-core.MontezumaRevengeSlow-v3),\n",
       " EnvSpec(SeaquestNoFrameskip-v0),\n",
       " EnvSpec(SeaquestNoFrameskip-v4),\n",
       " EnvSpec(flashgames.LearnToFlyIdle-v0),\n",
       " EnvSpec(gym-core.Asteroids30FPS-v0),\n",
       " EnvSpec(flashgames.JollySwipeLevelPack-v0),\n",
       " EnvSpec(gym-core.Asteroids30FPS-v3),\n",
       " EnvSpec(flashgames.Kinetikz3-v0),\n",
       " EnvSpec(flashgames.PiratesAndCannons-v0),\n",
       " EnvSpec(wob.real.Signup-14-v0),\n",
       " EnvSpec(flashgames.TaxiInc-v0),\n",
       " EnvSpec(flashgames.TankStorm2-v0),\n",
       " EnvSpec(gym-core.AirRaidDeterministicSync-v3),\n",
       " EnvSpec(gym-core.TennisSlow-v3),\n",
       " EnvSpec(gym-core.TennisSlow-v0),\n",
       " EnvSpec(gym-core.AirRaidDeterministicSync-v0),\n",
       " EnvSpec(flashgames.JetpackJackride-v0),\n",
       " EnvSpec(AirRaid-ram-v0),\n",
       " EnvSpec(Carnival-ram-v0),\n",
       " EnvSpec(AirRaid-ram-v4),\n",
       " EnvSpec(gym-core.AsteroidsDeterministicSlow-v0),\n",
       " EnvSpec(gym-core.AsteroidsDeterministicSlow-v3),\n",
       " EnvSpec(Carnival-ram-v4),\n",
       " EnvSpec(flashgames.MatchStars-v0),\n",
       " EnvSpec(flashgames.GunpowderAndFeathers-v0),\n",
       " EnvSpec(gym-core.VideoPinball-v0),\n",
       " EnvSpec(BeamRiderDeterministic-v0),\n",
       " EnvSpec(flashgames.TheTowerman-v0),\n",
       " EnvSpec(BeamRiderDeterministic-v4),\n",
       " EnvSpec(flashgames.FormulaRacerLvl5-v0),\n",
       " EnvSpec(gym-core.Centipede-v0),\n",
       " EnvSpec(gym-core.Centipede-v3),\n",
       " EnvSpec(Phoenix-ramDeterministic-v0),\n",
       " EnvSpec(gym-core.KrullSync-v3),\n",
       " EnvSpec(gym-core.KrullSync-v0),\n",
       " EnvSpec(gym-core.VentureSync-v0),\n",
       " EnvSpec(Phoenix-ramDeterministic-v4),\n",
       " EnvSpec(flashgames.BubbleShooterChallenge-v0),\n",
       " EnvSpec(flashgames.BubbleBlubbs-v0),\n",
       " EnvSpec(gym-core.KungFuMasterSync-v0),\n",
       " EnvSpec(gym-core.KungFuMasterSync-v3),\n",
       " EnvSpec(gym-core.AmidarSync-v3),\n",
       " EnvSpec(Boxing-ramDeterministic-v4),\n",
       " EnvSpec(gym-core.AmidarSync-v0),\n",
       " EnvSpec(Boxing-ramDeterministic-v0),\n",
       " EnvSpec(flashgames.WaveLucha-v0),\n",
       " EnvSpec(flashgames.EvolutionRacingLvl11-v0),\n",
       " EnvSpec(gym-core.ChopperCommandSync-v0),\n",
       " EnvSpec(gym-core.ElevatorActionDeterministicSlow-v0),\n",
       " EnvSpec(gym-core.ElevatorActionDeterministicSlow-v3),\n",
       " EnvSpec(flashgames.SpaceMadness-v0),\n",
       " EnvSpec(flashgames.KingRolla-v0),\n",
       " EnvSpec(gym-core.BerzerkSlow-v0),\n",
       " EnvSpec(Thrower-v0),\n",
       " EnvSpec(flashgames.BikeTrial3-v0),\n",
       " EnvSpec(flashgames.ChickCannont-v0),\n",
       " EnvSpec(flashgames.BearInSuperActionAdventure-v0),\n",
       " EnvSpec(gym-core.VideoPinballSync-v3),\n",
       " EnvSpec(gym-core.VideoPinballSync-v0),\n",
       " EnvSpec(flashgames.Thaw-v0),\n",
       " EnvSpec(flashgames.NewSiberianSupercarsRacing-v0),\n",
       " EnvSpec(flashgames.MinedigJourneyToHollowEarth-v0),\n",
       " EnvSpec(gym-core.BankHeistSync-v0),\n",
       " EnvSpec(gym-core.SpaceInvaders-v3),\n",
       " EnvSpec(flashgames.NeonRace2Lvl9-v0),\n",
       " EnvSpec(gym-core.NameThisGameDeterministicSync-v3),\n",
       " EnvSpec(gym-core.NameThisGameDeterministicSync-v0),\n",
       " EnvSpec(flashgames.Foosball2Player-v0),\n",
       " EnvSpec(gym-core.BattleZoneNoFrameskip-v0),\n",
       " EnvSpec(flashgames.SuperShinyheadHarderThanFlappyBird-v0),\n",
       " EnvSpec(gym-core.RoadRunnerNoFrameskip-v3),\n",
       " EnvSpec(gym-core.RoadRunnerNoFrameskip-v0),\n",
       " EnvSpec(gym-core.BreakoutSync-v0),\n",
       " EnvSpec(gym-core.BreakoutSync-v3),\n",
       " EnvSpec(flashgames.BullfrogJigsawPuzzle-v0),\n",
       " EnvSpec(flashgames.TheSilentPlanet-v0),\n",
       " EnvSpec(BerzerkDeterministic-v4),\n",
       " EnvSpec(flashgames.EuroKicks2016-v0),\n",
       " EnvSpec(wob.real.Quizlet-Solar-System-Learn-v0),\n",
       " EnvSpec(BerzerkDeterministic-v0),\n",
       " EnvSpec(AssaultNoFrameskip-v0),\n",
       " EnvSpec(flashgames.TouchTheSky-v0),\n",
       " EnvSpec(AssaultNoFrameskip-v4),\n",
       " EnvSpec(PhoenixNoFrameskip-v4),\n",
       " EnvSpec(gym-core.DoubleDunkSlow-v3),\n",
       " EnvSpec(PhoenixNoFrameskip-v0),\n",
       " EnvSpec(flashgames.IdleChop-v0),\n",
       " EnvSpec(gym-core.YarsRevenge-v3),\n",
       " EnvSpec(gym-core.AlienNoFrameskip-v0),\n",
       " EnvSpec(Humanoid-v1),\n",
       " EnvSpec(gym-core.AsteroidsSync-v3),\n",
       " EnvSpec(wob.mini.FindMidpoint-v0),\n",
       " EnvSpec(flashgames.DartsSim-v0),\n",
       " EnvSpec(flashgames.SmileyShowdown-v0),\n",
       " EnvSpec(flashgames.NeonRace2Lvl8-v0),\n",
       " EnvSpec(flashgames.SneakyScubaEscape-v0),\n",
       " EnvSpec(flashgames.SuperDash-v0),\n",
       " EnvSpec(flashgames.MummyMadness-v0),\n",
       " EnvSpec(gym-core.RobotankSlow-v0),\n",
       " EnvSpec(flashgames.HoldTheFort-v0),\n",
       " EnvSpec(KungFuMasterNoFrameskip-v0),\n",
       " EnvSpec(Frostbite-ramDeterministic-v4),\n",
       " EnvSpec(Frostbite-ramDeterministic-v0),\n",
       " EnvSpec(gym-core.VentureDeterministicSync-v3),\n",
       " EnvSpec(gym-core.VentureDeterministicSync-v0),\n",
       " EnvSpec(flashgames.FairyDefense-v0),\n",
       " EnvSpec(gym-core.RobotankSync-v0),\n",
       " EnvSpec(gym-core.RobotankSync-v3),\n",
       " EnvSpec(Qbert-ramNoFrameskip-v0),\n",
       " EnvSpec(Ant-v1),\n",
       " EnvSpec(Qbert-ramNoFrameskip-v4),\n",
       " EnvSpec(gym-core.Seaquest-v0),\n",
       " EnvSpec(gym-core.Seaquest-v3),\n",
       " EnvSpec(YarsRevenge-ram-v0),\n",
       " EnvSpec(YarsRevenge-ram-v4),\n",
       " EnvSpec(flashgames.IceBlock-v0),\n",
       " EnvSpec(FishingDerby-ram-v0),\n",
       " EnvSpec(Enduro-ramNoFrameskip-v4),\n",
       " EnvSpec(FrostbiteNoFrameskip-v0),\n",
       " EnvSpec(FishingDerby-ram-v4),\n",
       " EnvSpec(Enduro-ramNoFrameskip-v0),\n",
       " EnvSpec(FrostbiteNoFrameskip-v4),\n",
       " EnvSpec(gym-core.MsPacmanDeterministic-v3),\n",
       " EnvSpec(wob.mini.ClickColor-v0),\n",
       " EnvSpec(gym-core.MsPacmanDeterministic-v0),\n",
       " EnvSpec(flashgames.Dots-v0),\n",
       " EnvSpec(flashgames.NeonRace2Lvl6-v0),\n",
       " EnvSpec(gym-core.JamesbondDeterministicSlow-v3),\n",
       " EnvSpec(flashgames.NeonRace2Lvl12-v0),\n",
       " EnvSpec(gym-core.ElevatorActionDeterministicSync-v3),\n",
       " EnvSpec(gym-core.ElevatorActionDeterministicSync-v0),\n",
       " EnvSpec(flashgames.PixelPurge-v0),\n",
       " EnvSpec(flashgames.ReleaseTheMooks-v0),\n",
       " EnvSpec(gym-core.StarGunnerSlow-v3),\n",
       " EnvSpec(gym-core.StarGunnerSlow-v0),\n",
       " EnvSpec(flashgames.SapphireClix-v0),\n",
       " EnvSpec(gym-core.MontezumaRevengeDeterministicSync-v0),\n",
       " EnvSpec(flashgames.SlingBaby-v0),\n",
       " EnvSpec(gym-core.MontezumaRevengeDeterministicSync-v3),\n",
       " EnvSpec(gym-core.KungFuMasterDeterministicSlow-v3),\n",
       " EnvSpec(StarGunnerDeterministic-v0),\n",
       " EnvSpec(StarGunnerDeterministic-v4),\n",
       " EnvSpec(flashgames.SandcastleShowdown-v0),\n",
       " EnvSpec(flashgames.CharlieTheDuck-v0),\n",
       " EnvSpec(CNNClassifierTraining-v0),\n",
       " EnvSpec(wob.mini.EnterText-v0),\n",
       " EnvSpec(gym-core.BankHeistDeterministic-v3),\n",
       " EnvSpec(flashgames.CanyonValleyRally-v0),\n",
       " EnvSpec(gym-core.VentureDeterministic-v0),\n",
       " EnvSpec(Boxing-ram-v0),\n",
       " EnvSpec(Boxing-ram-v4),\n",
       " EnvSpec(gym-core.VentureDeterministic-v3),\n",
       " EnvSpec(flashgames.WackyStrike-v0),\n",
       " EnvSpec(flashgames.EasterEggSlider-v0),\n",
       " EnvSpec(flashgames.MindImpulse-v0),\n",
       " EnvSpec(flashgames.NeonRace2Lvl13-v0),\n",
       " EnvSpec(flashgames.FormulaRacer-v0),\n",
       " EnvSpec(flashgames.Hamsterball-v0),\n",
       " EnvSpec(gym-core.KungFuMasterDeterministicSync-v0),\n",
       " EnvSpec(Assault-ram-v0),\n",
       " EnvSpec(wob.mini.NumberCheckboxes-v0),\n",
       " EnvSpec(Assault-ram-v4),\n",
       " EnvSpec(gym-core.BreakoutDeterministic-v0),\n",
       " EnvSpec(gym-core.BreakoutDeterministic-v3),\n",
       " EnvSpec(flashgames.Offroaders2-v0),\n",
       " EnvSpec(gym-core.JamesbondDeterministicSync-v3),\n",
       " EnvSpec(gym-core.JamesbondDeterministicSync-v0),\n",
       " EnvSpec(flashgames.SnowQueen4-v0),\n",
       " EnvSpec(flashgames.ToyWarAngryRobotDog-v0),\n",
       " EnvSpec(flashgames.HighwayRevenge-v0),\n",
       " EnvSpec(flashgames.CarrotFantasyExtreme3-v0),\n",
       " EnvSpec(Solaris-ramDeterministic-v4),\n",
       " EnvSpec(ElevatorActionDeterministic-v4),\n",
       " EnvSpec(Solaris-ramDeterministic-v0),\n",
       " EnvSpec(ElevatorActionDeterministic-v0),\n",
       " EnvSpec(Solaris-v4),\n",
       " EnvSpec(Solaris-v0),\n",
       " EnvSpec(flashgames.BlockysEscape-v0),\n",
       " EnvSpec(gym-core.StarGunnerDeterministic-v3),\n",
       " EnvSpec(flashgames.HeroesOfMangaraTheFrostCrown-v0),\n",
       " EnvSpec(SolarisNoFrameskip-v0),\n",
       " EnvSpec(gym-core.PongDeterministic-v0),\n",
       " EnvSpec(SolarisNoFrameskip-v4),\n",
       " EnvSpec(RoadRunner-v4),\n",
       " EnvSpec(flashgames.ShortCircuit-v0),\n",
       " EnvSpec(RoadRunner-v0),\n",
       " EnvSpec(gym-core.AssaultDeterministicSlow-v0),\n",
       " EnvSpec(flashgames.SupercarDomination-v0),\n",
       " EnvSpec(gym-core.AssaultDeterministicSlow-v3),\n",
       " EnvSpec(flashgames.Xmatch2016-v0),\n",
       " EnvSpec(RoadRunner-ram-v0),\n",
       " EnvSpec(flashgames.CrystalCurse-v0),\n",
       " EnvSpec(RoadRunner-ram-v4),\n",
       " EnvSpec(gym-core.AsteroidsSlow-v3),\n",
       " EnvSpec(gym-core.AsteroidsSlow-v0),\n",
       " EnvSpec(Skiing-ramDeterministic-v0),\n",
       " EnvSpec(Skiing-ramDeterministic-v4),\n",
       " EnvSpec(flashgames.BlackRacerJigsawPuzzle-v0),\n",
       " EnvSpec(VideoPinball-ram-v0),\n",
       " EnvSpec(VideoPinball-ram-v4),\n",
       " EnvSpec(flashgames.DaymareInvaders-v0),\n",
       " EnvSpec(gym-core.GravitarNoFrameskip-v0),\n",
       " EnvSpec(FreewayNoFrameskip-v4),\n",
       " EnvSpec(WizardOfWorNoFrameskip-v0),\n",
       " EnvSpec(FreewayNoFrameskip-v0),\n",
       " EnvSpec(WizardOfWorNoFrameskip-v4),\n",
       " EnvSpec(flashgames.UnderwaterSecrets-v0),\n",
       " EnvSpec(gym-core.ElevatorAction30FPS-v0),\n",
       " EnvSpec(gym-core.ElevatorAction30FPS-v3),\n",
       " EnvSpec(flashgames.OkParking-v0),\n",
       " EnvSpec(flashgames.HeatRushFuture-v0),\n",
       " EnvSpec(gym-core.Venture-v0),\n",
       " EnvSpec(DoubleDunk-v4),\n",
       " EnvSpec(gym-core.MsPacmanDeterministicSync-v0),\n",
       " EnvSpec(gym-core.MsPacmanDeterministicSync-v3),\n",
       " EnvSpec(flashgames.WorldsGuard2-v0),\n",
       " EnvSpec(gym-core.BattleZoneDeterministicSync-v3),\n",
       " EnvSpec(gym-core.BattleZoneDeterministicSync-v0),\n",
       " EnvSpec(gym-core.VideoPinballNoFrameskip-v0),\n",
       " EnvSpec(gym-core.VideoPinballNoFrameskip-v3),\n",
       " EnvSpec(gym-core.JourneyEscapeNoFrameskip-v0),\n",
       " EnvSpec(HeroDeterministic-v4),\n",
       " EnvSpec(gym-core.JourneyEscapeNoFrameskip-v3),\n",
       " EnvSpec(HeroDeterministic-v0),\n",
       " EnvSpec(ZaxxonDeterministic-v0),\n",
       " EnvSpec(gym-core.PrivateEyeDeterministic-v0),\n",
       " EnvSpec(gym-core.PrivateEyeDeterministic-v3),\n",
       " EnvSpec(flashgames.Colorwars-v0),\n",
       " EnvSpec(gym-core.MsPacmanNoFrameskip-v0),\n",
       " EnvSpec(gym-core.JamesbondSlow-v0),\n",
       " EnvSpec(gym-core.JamesbondSlow-v3),\n",
       " EnvSpec(gym-core.MsPacmanNoFrameskip-v3),\n",
       " EnvSpec(gym-core.RiverraidSlow-v3),\n",
       " EnvSpec(gym-core.CartPole-v0),\n",
       " EnvSpec(flashgames.GalaxyMission-v0),\n",
       " EnvSpec(BeamRider-v4),\n",
       " EnvSpec(flashgames.SuperBoxotron2000-v0),\n",
       " EnvSpec(FishingDerby-ramDeterministic-v0),\n",
       " EnvSpec(FishingDerby-ramDeterministic-v4),\n",
       " EnvSpec(flashgames.Stratega-v0),\n",
       " EnvSpec(gym-core.AsteroidsNoFrameskip-v3),\n",
       " EnvSpec(CrazyClimber-v4),\n",
       " EnvSpec(flashgames.ParticleWarsExtreme-v0),\n",
       " EnvSpec(gym-core.AsteroidsNoFrameskip-v0),\n",
       " EnvSpec(CrazyClimber-v0),\n",
       " EnvSpec(flashgames.HexBattles-v0),\n",
       " EnvSpec(gym-core.BoxingNoFrameskip-v3),\n",
       " EnvSpec(Pitfall-ramDeterministic-v0),\n",
       " EnvSpec(flashgames.BumbleTumble-v0),\n",
       " EnvSpec(gym-core.BoxingNoFrameskip-v0),\n",
       " EnvSpec(Pitfall-ramDeterministic-v4),\n",
       " EnvSpec(wob.real.Quizlet-Geography-Test-v0),\n",
       " EnvSpec(gym-core.SkiingSync-v0),\n",
       " EnvSpec(gym-core.SkiingSync-v3),\n",
       " EnvSpec(GravitarDeterministic-v4),\n",
       " EnvSpec(flashgames.CoasterRacerLvl5-v0),\n",
       " EnvSpec(GravitarDeterministic-v0),\n",
       " EnvSpec(gym-core.BowlingNoFrameskip-v3),\n",
       " EnvSpec(gym-core.BowlingNoFrameskip-v0),\n",
       " EnvSpec(flashgames.EvilSun-v0),\n",
       " EnvSpec(flashgames.HalloweenJam-v0),\n",
       " EnvSpec(flashgames.LlamasInDistress-v0),\n",
       " EnvSpec(gym-core.PooyanDeterministic-v0),\n",
       " EnvSpec(flashgames.WreckRoad-v0),\n",
       " EnvSpec(wob.real.Signup-6-v0),\n",
       " EnvSpec(flashgames.EvolutionRacingLvl9-v0),\n",
       " EnvSpec(gym-core.ElevatorActionSlow-v0),\n",
       " EnvSpec(gym-core.ElevatorActionSlow-v3),\n",
       " EnvSpec(wob.real.Signup-7-v0),\n",
       " EnvSpec(flashgames.SistersOfNoMercy-v0),\n",
       " EnvSpec(Tennis-v0),\n",
       " EnvSpec(flashgames.CoasterRacerLvl4-v0),\n",
       " EnvSpec(flashgames.EvolutionRacingLvl4-v0),\n",
       " EnvSpec(gym-core.RoadRunner-v0),\n",
       " EnvSpec(gym-core.RoadRunner-v3),\n",
       " EnvSpec(Carnival-ramDeterministic-v4),\n",
       " EnvSpec(gym-core.Carnival-v0),\n",
       " EnvSpec(flashgames.CursedTreasureDontTouchMyGems-v0),\n",
       " EnvSpec(gym-core.Carnival-v3),\n",
       " EnvSpec(flashgames.FlappyBat-v0),\n",
       " EnvSpec(wob.mini.ResizeTextarea-v0),\n",
       " EnvSpec(gym-core.SpaceInvadersNoFrameskip-v3),\n",
       " EnvSpec(TennisNoFrameskip-v0),\n",
       " EnvSpec(gym-core.PhoenixNoFrameskip-v3),\n",
       " EnvSpec(gym-core.PhoenixNoFrameskip-v0),\n",
       " EnvSpec(gym-core.Alien30FPS-v3),\n",
       " EnvSpec(gym-core.Alien30FPS-v0),\n",
       " EnvSpec(flashgames.TheOneForkRestaurantDx-v0),\n",
       " EnvSpec(DemonAttack-ram-v0),\n",
       " EnvSpec(DemonAttack-ram-v4),\n",
       " EnvSpec(gym-core.KungFuMasterNoFrameskip-v0),\n",
       " EnvSpec(gym-core.KungFuMasterNoFrameskip-v3),\n",
       " EnvSpec(flashgames.SpacePunkRacer-v0),\n",
       " EnvSpec(TennisNoFrameskip-v4),\n",
       " EnvSpec(flashgames.FishAndDestroy-v0),\n",
       " EnvSpec(flashgames.KartRacing-v0),\n",
       " EnvSpec(flashgames.JellySnake-v0),\n",
       " EnvSpec(gym-core.AsterixSlow-v0),\n",
       " EnvSpec(gym-core.AsterixSlow-v3),\n",
       " EnvSpec(flashgames.FishEatFish-v0),\n",
       " EnvSpec(flashgames.Jumprunner-v0),\n",
       " EnvSpec(flashgames.HoleInOne-v0),\n",
       " EnvSpec(flashgames.AwesomeRun2-v0),\n",
       " EnvSpec(gym-core.BattleZoneSlow-v0),\n",
       " EnvSpec(gym-core.BattleZoneSlow-v3),\n",
       " EnvSpec(flashgames.IntoSpace-v0),\n",
       " EnvSpec(flashgames.CarsVsRobots-v0),\n",
       " EnvSpec(flashgames.BubbleHitPonyParade-v0),\n",
       " EnvSpec(flashgames.AWeekendAtTweetys-v0),\n",
       " EnvSpec(gym-core.DoubleDunkDeterministicSync-v0),\n",
       " EnvSpec(gym-core.DoubleDunkDeterministicSync-v3),\n",
       " EnvSpec(Amidar-v4),\n",
       " EnvSpec(flashgames.ModelCarRacing-v0),\n",
       " EnvSpec(gym-core.ChopperCommandDeterministicSync-v3),\n",
       " EnvSpec(gym-core.ChopperCommandDeterministicSync-v0),\n",
       " EnvSpec(flashgames.PirateRunAway-v0),\n",
       " EnvSpec(flashgames.MonsterLabFeedThemAll-v0),\n",
       " EnvSpec(Gravitar-ramDeterministic-v0),\n",
       " EnvSpec(gym-core.DemonAttack30FPS-v0),\n",
       " EnvSpec(flashgames.ViewtifulFightClub2-v0),\n",
       " EnvSpec(Gravitar-ramDeterministic-v4),\n",
       " EnvSpec(BattleZone-ram-v4),\n",
       " EnvSpec(BattleZone-ram-v0),\n",
       " EnvSpec(IceHockey-ram-v0),\n",
       " EnvSpec(IceHockey-ram-v4),\n",
       " EnvSpec(flashgames.DragonChronicles-v0),\n",
       " EnvSpec(gym-core.PitfallDeterministic-v0),\n",
       " EnvSpec(gym-core.PitfallDeterministic-v3),\n",
       " EnvSpec(flashgames.SnowPrincessMakeup-v0),\n",
       " EnvSpec(flashgames.QubeyTheCube-v0),\n",
       " EnvSpec(gym-core.Alien-v3),\n",
       " EnvSpec(gym-core.Alien-v0),\n",
       " EnvSpec(gym-core.AtlantisSync-v0),\n",
       " EnvSpec(gym-core.AtlantisSync-v3),\n",
       " EnvSpec(flashgames.TowerMoon-v0),\n",
       " EnvSpec(flashgames.MotherLoad-v0),\n",
       " EnvSpec(wob.mini.EnterTextDynamic-v0),\n",
       " EnvSpec(flashgames.BubbleGlee-v0),\n",
       " EnvSpec(flashgames.FirefighterCannon-v0),\n",
       " EnvSpec(gym-core.BerzerkDeterministicSlow-v0),\n",
       " EnvSpec(gym-core.BerzerkDeterministicSlow-v3),\n",
       " EnvSpec(DoubleDunk-ramNoFrameskip-v0),\n",
       " EnvSpec(gym-core.IceHockeySlow-v3),\n",
       " EnvSpec(gym-core.IceHockeySlow-v0),\n",
       " EnvSpec(AssaultDeterministic-v0),\n",
       " EnvSpec(DoubleDunk-ramNoFrameskip-v4),\n",
       " EnvSpec(MsPacman-v0),\n",
       " EnvSpec(flashgames.Offroaders-v0),\n",
       " EnvSpec(MsPacman-v4),\n",
       " EnvSpec(flashgames.HiredHeroes-v0),\n",
       " EnvSpec(flashgames.WolfSpiderJigsawPuzzle-v0),\n",
       " EnvSpec(AssaultDeterministic-v4),\n",
       " EnvSpec(gym-core.FreewayDeterministic-v3),\n",
       " EnvSpec(flashgames.ToonEscapeMaze-v0),\n",
       " EnvSpec(wob.mini.ClickCheckboxes-v0),\n",
       " EnvSpec(gym-core.FreewayDeterministic-v0),\n",
       " EnvSpec(Seaquest-ramNoFrameskip-v0),\n",
       " EnvSpec(Seaquest-ramNoFrameskip-v4),\n",
       " EnvSpec(Blackjack-v0),\n",
       " EnvSpec(TennisDeterministic-v0),\n",
       " EnvSpec(TennisDeterministic-v4),\n",
       " EnvSpec(Atlantis-v4),\n",
       " EnvSpec(Atlantis-v0),\n",
       " EnvSpec(UpNDownDeterministic-v0),\n",
       " EnvSpec(flashgames.WarBerlinIdle-v0),\n",
       " EnvSpec(gym-core.BattleZoneSync-v3),\n",
       " EnvSpec(gym-core.Centipede30FPS-v0),\n",
       " EnvSpec(gym-core.Centipede30FPS-v3),\n",
       " EnvSpec(gym-core.BattleZoneSync-v0),\n",
       " EnvSpec(gym-core.FishingDerby-v0),\n",
       " EnvSpec(Asteroids-v0),\n",
       " EnvSpec(Asteroids-v4),\n",
       " EnvSpec(gym-core.SpaceInvaders30FPS-v0),\n",
       " EnvSpec(UpNDownDeterministic-v4),\n",
       " EnvSpec(IceHockeyDeterministic-v4),\n",
       " EnvSpec(flashgames.SpectrumHeist-v0),\n",
       " EnvSpec(gym-core.BattleZoneNoFrameskip-v3),\n",
       " EnvSpec(IceHockeyDeterministic-v0),\n",
       " EnvSpec(gym-core.EnduroNoFrameskip-v3),\n",
       " EnvSpec(gym-core.EnduroNoFrameskip-v0),\n",
       " EnvSpec(flashgames.TankStorm3-v0),\n",
       " EnvSpec(gym-core.BeamRiderDeterministic-v0),\n",
       " EnvSpec(gym-core.BeamRiderDeterministic-v3),\n",
       " EnvSpec(flashgames.Infinitix-v0),\n",
       " EnvSpec(flashgames.PoliceInterceptor-v0),\n",
       " EnvSpec(gym-core.CrazyClimber-v3),\n",
       " EnvSpec(wob.real.ClickButton-Airfrance-v0),\n",
       " EnvSpec(gym-core.CrazyClimber-v0),\n",
       " EnvSpec(flashgames.Autoattack-v0),\n",
       " EnvSpec(flashgames.CircuitSuperCarsRacing-v0),\n",
       " EnvSpec(flashgames.HeatRushUsaLvl8-v0),\n",
       " EnvSpec(flashgames.Blix-v0),\n",
       " EnvSpec(WizardOfWorDeterministic-v4),\n",
       " EnvSpec(gym-core.RoadRunnerDeterministic-v0),\n",
       " EnvSpec(gym-core.RoadRunnerDeterministic-v3),\n",
       " EnvSpec(WizardOfWorDeterministic-v0),\n",
       " EnvSpec(gym-core.IceHockeyNoFrameskip-v3),\n",
       " EnvSpec(gym-core.IceHockeyNoFrameskip-v0),\n",
       " EnvSpec(flashgames.ElClassico-v0),\n",
       " EnvSpec(DoubleDunk-v0),\n",
       " EnvSpec(LunarLander-v2),\n",
       " EnvSpec(MsPacman-ramDeterministic-v4),\n",
       " EnvSpec(flashgames.Neopods-v0),\n",
       " EnvSpec(MsPacman-ramDeterministic-v0),\n",
       " EnvSpec(flashgames.TutiFruti-v0),\n",
       " EnvSpec(flashgames.WhatsInsideTheBox-v0),\n",
       " EnvSpec(BoxingDeterministic-v0),\n",
       " EnvSpec(BoxingDeterministic-v4),\n",
       " EnvSpec(gym-core.PooyanDeterministicSlow-v3),\n",
       " EnvSpec(gym-core.PooyanDeterministicSlow-v0),\n",
       " EnvSpec(flashgames.3dMuscleCarRacer-v0),\n",
       " EnvSpec(flashgames.ColorZapper-v0),\n",
       " EnvSpec(Robotank-v4),\n",
       " EnvSpec(Robotank-v0),\n",
       " EnvSpec(flashgames.HeroSimulator-v0),\n",
       " EnvSpec(wob.mini.ClickButton-v0),\n",
       " EnvSpec(wob.mini.SimpleArithmetic-v0),\n",
       " EnvSpec(flashgames.FormulaRacer2012Lvl11-v0),\n",
       " EnvSpec(flashgames.SuperRallyChallenge2-v0),\n",
       " EnvSpec(flashgames.AmericanRacing2-v0),\n",
       " EnvSpec(gym-core.SolarisSync-v0),\n",
       " EnvSpec(gym-core.SolarisSync-v3),\n",
       " EnvSpec(Gravitar-ram-v0),\n",
       " EnvSpec(Frostbite-v4),\n",
       " EnvSpec(Gravitar-ram-v4),\n",
       " EnvSpec(Frostbite-v0),\n",
       " EnvSpec(Acrobot-v1),\n",
       " EnvSpec(gym-core.FrostbiteDeterministic-v3),\n",
       " EnvSpec(gym-core.FrostbiteDeterministic-v0),\n",
       " EnvSpec(wob.mini.ClickDialog-v0),\n",
       " EnvSpec(flashgames.Wheelers-v0),\n",
       " EnvSpec(starcraft.TerranAstralBalance-v0),\n",
       " EnvSpec(ZaxxonNoFrameskip-v0),\n",
       " EnvSpec(HeroNoFrameskip-v4),\n",
       " EnvSpec(flashgames.BubbleSlasher-v0),\n",
       " EnvSpec(ZaxxonNoFrameskip-v4),\n",
       " EnvSpec(flashgames.AmericanRacingLvl3-v0),\n",
       " EnvSpec(HeroNoFrameskip-v0),\n",
       " EnvSpec(NameThisGame-v0),\n",
       " EnvSpec(flashgames.FormulaRacer2012Lvl10-v0),\n",
       " EnvSpec(flashgames.CoverOrangeJourneyGangsters-v0),\n",
       " EnvSpec(NameThisGame-v4),\n",
       " EnvSpec(flashgames.PaintWars-v0),\n",
       " EnvSpec(gym-core.StarGunner-v0),\n",
       " EnvSpec(flashgames.AchilliaTheGame-v0),\n",
       " EnvSpec(gym-core.StarGunner-v3),\n",
       " EnvSpec(flashgames.KnightsOfRock-v0),\n",
       " EnvSpec(gym-core.Jamesbond30FPS-v0),\n",
       " EnvSpec(gym-core.Jamesbond30FPS-v3),\n",
       " EnvSpec(flashgames.GalacticCats-v0),\n",
       " EnvSpec(Krull-ram-v0),\n",
       " EnvSpec(Krull-ram-v4),\n",
       " EnvSpec(flashgames.DeathDiceOverdose-v0),\n",
       " EnvSpec(flashgames.BubbleRubble-v0),\n",
       " EnvSpec(BowlingNoFrameskip-v0),\n",
       " EnvSpec(flashgames.ImitationNationSnakeGame-v0),\n",
       " EnvSpec(gym-core.CentipedeSync-v3),\n",
       " EnvSpec(BowlingNoFrameskip-v4),\n",
       " EnvSpec(gym-core.CentipedeSync-v0),\n",
       " EnvSpec(flashgames.IceRun-v0),\n",
       " EnvSpec(flashgames.Madburger3-v0),\n",
       " EnvSpec(gym-core.MontezumaRevengeDeterministic-v3),\n",
       " EnvSpec(flashgames.GsSoccerWorldCup-v0),\n",
       " EnvSpec(gym-core.MontezumaRevengeDeterministic-v0),\n",
       " EnvSpec(flashgames.NeonRaceLvl3-v0),\n",
       " EnvSpec(Assault-ramNoFrameskip-v0),\n",
       " EnvSpec(Assault-ramNoFrameskip-v4),\n",
       " EnvSpec(BankHeist-ram-v0),\n",
       " EnvSpec(flashgames.HungryPiranha-v0),\n",
       " EnvSpec(BankHeist-ram-v4),\n",
       " EnvSpec(flashgames.DoodleGod2Walkthrough-v0),\n",
       " EnvSpec(SpaceInvaders-ram-v0),\n",
       " EnvSpec(SpaceInvaders-ram-v4),\n",
       " EnvSpec(FishingDerby-v4),\n",
       " EnvSpec(FishingDerby-v0),\n",
       " EnvSpec(flashgames.TheBoomlandsWorldWars-v0),\n",
       " EnvSpec(gym-core.GravitarDeterministicSync-v0),\n",
       " EnvSpec(Freeway-ramDeterministic-v4),\n",
       " EnvSpec(gym-core.GravitarDeterministicSync-v3),\n",
       " EnvSpec(DoubleDunkNoFrameskip-v0),\n",
       " EnvSpec(flashgames.Helixteus-v0),\n",
       " EnvSpec(DoubleDunkNoFrameskip-v4),\n",
       " EnvSpec(flashgames.SuperPuzzlePlatformer-v0),\n",
       " EnvSpec(flashgames.MatchAndCrash-v0),\n",
       " EnvSpec(AsterixNoFrameskip-v0),\n",
       " EnvSpec(AsterixNoFrameskip-v4),\n",
       " EnvSpec(flashgames.MatchCraft-v0),\n",
       " EnvSpec(SpaceInvaders-ramDeterministic-v4),\n",
       " EnvSpec(SpaceInvaders-ramDeterministic-v0),\n",
       " EnvSpec(JourneyEscapeDeterministic-v0),\n",
       " EnvSpec(flashgames.HandsOff-v0),\n",
       " EnvSpec(flashgames.Zevil2-v0),\n",
       " EnvSpec(JourneyEscapeDeterministic-v4),\n",
       " EnvSpec(flashgames.Paintwars-v0),\n",
       " EnvSpec(gym-core.PooyanSync-v0),\n",
       " EnvSpec(gym-core.PooyanSync-v3),\n",
       " EnvSpec(flashgames.MasterDifference-v0),\n",
       " EnvSpec(BeamRider-ram-v0),\n",
       " EnvSpec(BeamRider-ram-v4),\n",
       " EnvSpec(gym-core.ChopperCommandSlow-v0),\n",
       " EnvSpec(gym-core.ChopperCommandSlow-v3),\n",
       " EnvSpec(gym-core.Bowling30FPS-v0),\n",
       " EnvSpec(gym-core.Bowling30FPS-v3),\n",
       " EnvSpec(flashgames.Overheat-v0),\n",
       " EnvSpec(flashgames.GravityThruster-v0),\n",
       " EnvSpec(flashgames.NeonRaceLvl2-v0),\n",
       " EnvSpec(gtav.Speed-v0),\n",
       " EnvSpec(flashgames.TowerEmpire-v0),\n",
       " EnvSpec(Freeway-ramNoFrameskip-v4),\n",
       " EnvSpec(Freeway-ramNoFrameskip-v0),\n",
       " EnvSpec(PitfallDeterministic-v4),\n",
       " EnvSpec(flashgames.FormulaRacerLvl6-v0),\n",
       " EnvSpec(flashgames.HappyBallz-v0),\n",
       " EnvSpec(PitfallDeterministic-v0),\n",
       " EnvSpec(flashgames.Flagman-v0),\n",
       " EnvSpec(flashgames.PiggysCupcakeQuest-v0),\n",
       " EnvSpec(gym-core.KungFuMaster-v3),\n",
       " EnvSpec(gym-core.KungFuMaster-v0),\n",
       " EnvSpec(flashgames.AmericanRacingLvl21-v0),\n",
       " EnvSpec(flashgames.CoasterRacer-v0),\n",
       " EnvSpec(gym-core.PooyanNoFrameskip-v0),\n",
       " EnvSpec(flashgames.JungleCrash-v0),\n",
       " EnvSpec(gym-core.PooyanNoFrameskip-v3),\n",
       " EnvSpec(gym-core.RiverraidSync-v0),\n",
       " EnvSpec(gym-core.RiverraidSync-v3),\n",
       " EnvSpec(flashgames.ToyRacers-v0),\n",
       " EnvSpec(gym-core.JamesbondNoFrameskip-v0),\n",
       " EnvSpec(flashgames.DrinkBeerNeglectFamily-v0),\n",
       " EnvSpec(gym-core.ZaxxonDeterministic-v0),\n",
       " EnvSpec(gym-core.ZaxxonDeterministic-v3),\n",
       " EnvSpec(gym-core.Seaquest30FPS-v3),\n",
       " EnvSpec(gym-core.Seaquest30FPS-v0),\n",
       " EnvSpec(flashgames.FormulaRacer2012Lvl6-v0),\n",
       " EnvSpec(flashgames.TheCubicMonkeyAdventures2-v0),\n",
       " EnvSpec(flashgames.PlaneRace2-v0),\n",
       " EnvSpec(flashgames.Cruisin-v0),\n",
       " EnvSpec(DuplicatedInput-v0),\n",
       " EnvSpec(flashgames.WarOfTheShard-v0),\n",
       " EnvSpec(flashgames.UnfreezeMe3-v0),\n",
       " EnvSpec(flashgames.AnotherLife2-v0),\n",
       " EnvSpec(flashgames.HeatRushUsaLvl2-v0),\n",
       " EnvSpec(wob.mini.EnterTime-v0),\n",
       " EnvSpec(DemonAttack-ramNoFrameskip-v0),\n",
       " EnvSpec(gym-core.StarGunnerNoFrameskip-v3),\n",
       " EnvSpec(DemonAttack-ramNoFrameskip-v4),\n",
       " EnvSpec(flashgames.ExperimentalShooter2-v0),\n",
       " EnvSpec(flashgames.DodgeAndCrash-v0),\n",
       " EnvSpec(flashgames.BoxRacers-v0),\n",
       " EnvSpec(flashgames.IndependenceDaySlacking2015-v0),\n",
       " EnvSpec(flashgames.UltimateLegend-v0),\n",
       " EnvSpec(gym-core.CarnivalSlow-v0),\n",
       " EnvSpec(gym-core.CarnivalSlow-v3),\n",
       " EnvSpec(flashgames.RainbowDrops-v0),\n",
       " EnvSpec(gym-core.KungFuMasterDeterministic-v3),\n",
       " EnvSpec(gym-core.FreewaySync-v0),\n",
       " EnvSpec(gym-core.FreewaySync-v3),\n",
       " EnvSpec(gym-core.KungFuMasterDeterministic-v0),\n",
       " EnvSpec(flashgames.HyperTravel-v0),\n",
       " EnvSpec(flashgames.RiseOfChampions-v0),\n",
       " EnvSpec(flashgames.NadiasRage-v0),\n",
       " EnvSpec(flashgames.TrickyRick-v0),\n",
       " EnvSpec(flashgames.FredFigglehorn-v0),\n",
       " EnvSpec(flashgames.FormulaRacer2012Lvl7-v0),\n",
       " EnvSpec(flashgames.FlyingKiwi-v0),\n",
       " EnvSpec(flashgames.PickUpTruckRacing-v0),\n",
       " EnvSpec(flashgames.RhythmBlasterV2-v0),\n",
       " EnvSpec(flashgames.HeavyLegion2-v0),\n",
       " EnvSpec(gym-core.PrivateEyeDeterministicSlow-v0),\n",
       " EnvSpec(gym-core.AlienDeterministicSlow-v3),\n",
       " EnvSpec(gym-core.AlienDeterministicSlow-v0),\n",
       " EnvSpec(gym-core.PrivateEyeDeterministicSlow-v3),\n",
       " EnvSpec(gym-core.BattleZoneDeterministicSlow-v0),\n",
       " EnvSpec(flashgames.MushyMishy-v0),\n",
       " EnvSpec(flashgames.BombIt4-v0),\n",
       " EnvSpec(flashgames.MonkeyGems-v0),\n",
       " EnvSpec(gym-core.YarsRevengeDeterministicSync-v0),\n",
       " EnvSpec(flashgames.TechnoMania-v0),\n",
       " EnvSpec(gym-core.YarsRevengeDeterministicSync-v3),\n",
       " EnvSpec(flashgames.Mushbooms-v0),\n",
       " EnvSpec(RiverraidDeterministic-v4),\n",
       " EnvSpec(gym-core.BattleZoneDeterministicSlow-v3),\n",
       " EnvSpec(RiverraidDeterministic-v0),\n",
       " EnvSpec(flashgames.TumbleTiles-v0),\n",
       " EnvSpec(wob.mini.ChooseList-v0),\n",
       " EnvSpec(flashgames.Basement-v0),\n",
       " EnvSpec(flashgames.BikeTrial4-v0),\n",
       " EnvSpec(gym-core.CarnivalDeterministicSlow-v0),\n",
       " EnvSpec(gym-core.CarnivalDeterministicSlow-v3),\n",
       " EnvSpec(gym-core.CrazyClimberNoFrameskip-v3),\n",
       " EnvSpec(gym-core.PrivateEyeDeterministicSync-v3),\n",
       " EnvSpec(gym-core.PrivateEyeDeterministicSync-v0),\n",
       " EnvSpec(flashgames.HeavenAndHell-v0),\n",
       " EnvSpec(flashgames.JamesTheSpaceZebra-v0),\n",
       " EnvSpec(flashgames.GoGreenGo-v0),\n",
       " EnvSpec(KellyCoinflipGeneralized-v0),\n",
       " EnvSpec(gym-core.UpNDown-v0),\n",
       " EnvSpec(flashgames.CaptainNutty-v0),\n",
       " EnvSpec(flashgames.TheProfessionals3-v0),\n",
       " EnvSpec(gym-core.SolarisNoFrameskip-v0),\n",
       " EnvSpec(gym-core.SolarisNoFrameskip-v3),\n",
       " EnvSpec(flashgames.PickAndDig2-v0),\n",
       " EnvSpec(flashgames.EasterBunnyCollectCarrots-v0),\n",
       " EnvSpec(Gopher-ram-v0),\n",
       " EnvSpec(flashgames.Tosuta-v0),\n",
       " EnvSpec(Gopher-ram-v4),\n",
       " EnvSpec(flashgames.StickyNinjaMissions-v0),\n",
       " EnvSpec(CarRacing-v0),\n",
       " EnvSpec(flashgames.DiscoverEurope-v0),\n",
       " EnvSpec(gym-core.VideoPinballDeterministicSync-v3),\n",
       " EnvSpec(gym-core.BerzerkSync-v3),\n",
       " EnvSpec(flashgames.4x4Monster3-v0),\n",
       " EnvSpec(wob.mini.CopyPaste2-v0),\n",
       " EnvSpec(gym-core.BerzerkSync-v0),\n",
       " EnvSpec(AmidarNoFrameskip-v0),\n",
       " EnvSpec(flashgames.BubbleTanksTd15-v0),\n",
       " EnvSpec(AmidarNoFrameskip-v4),\n",
       " EnvSpec(gym-core.CrazyClimberDeterministic-v3),\n",
       " EnvSpec(flashgames.LuxUltimate-v0),\n",
       " EnvSpec(flashgames.ZodiacMatch-v0),\n",
       " EnvSpec(gym-core.Riverraid30FPS-v0),\n",
       " EnvSpec(gym-core.AirRaidDeterministicSlow-v0),\n",
       " EnvSpec(gym-core.AirRaidDeterministicSlow-v3),\n",
       " EnvSpec(flashgames.Cloud9-v0),\n",
       " EnvSpec(gym-core.WizardOfWorDeterministic-v3),\n",
       " EnvSpec(gym-core.WizardOfWorDeterministic-v0),\n",
       " EnvSpec(Asteroids-ram-v0),\n",
       " EnvSpec(gym-core.MsPacmanSlow-v0),\n",
       " EnvSpec(Asteroids-ram-v4),\n",
       " EnvSpec(gym-core.ZaxxonNoFrameskip-v3),\n",
       " EnvSpec(gym-core.Enduro-v3),\n",
       " EnvSpec(gym-core.Enduro-v0),\n",
       " EnvSpec(gym-core.ZaxxonNoFrameskip-v0),\n",
       " EnvSpec(flashgames.NewSplitterPals-v0),\n",
       " EnvSpec(flashgames.21Balloons-v0),\n",
       " EnvSpec(flashgames.Match3Adventure-v0),\n",
       " EnvSpec(flashgames.DragonVsMonster-v0),\n",
       " EnvSpec(flashgames.HeatRushFutureLvl5-v0),\n",
       " EnvSpec(Solaris-ram-v0),\n",
       " EnvSpec(Solaris-ram-v4),\n",
       " EnvSpec(flashgames.EpicDefender-v0),\n",
       " EnvSpec(flashgames.DragonChain-v0),\n",
       " EnvSpec(flashgames.FootballHeads201314Ligue1-v0),\n",
       " EnvSpec(flashgames.LonelyEscapeAsylum-v0),\n",
       " EnvSpec(MontezumaRevengeDeterministic-v0),\n",
       " EnvSpec(flashgames.MonkeyBlast-v0),\n",
       " EnvSpec(wob.real.BookFlight-Delta-v0),\n",
       " EnvSpec(ReversedAddition-v0),\n",
       " EnvSpec(gym-core.YarsRevengeSlow-v3),\n",
       " EnvSpec(gym-core.YarsRevengeSlow-v0),\n",
       " EnvSpec(wob.mini.EmailInbox-v0),\n",
       " EnvSpec(flashgames.VectorRunner-v0),\n",
       " EnvSpec(RiverraidNoFrameskip-v4),\n",
       " EnvSpec(RiverraidNoFrameskip-v0),\n",
       " EnvSpec(flashgames.AmericanRacingLvl13-v0),\n",
       " EnvSpec(BipedalWalkerHardcore-v2),\n",
       " EnvSpec(flashgames.AmericanRacingLvl22-v0),\n",
       " EnvSpec(gym-core.PrivateEye-v0),\n",
       " EnvSpec(gym-core.PrivateEye-v3),\n",
       " EnvSpec(flashgames.CoasterCars2Megacross-v0),\n",
       " EnvSpec(flashgames.SkyIsland-v0),\n",
       " EnvSpec(flashgames.SuperbikeExtreme-v0),\n",
       " EnvSpec(gym-core.KangarooSync-v3),\n",
       " EnvSpec(flashgames.SpacePunkRacerLvl5-v0),\n",
       " EnvSpec(flashgames.MarshmallowsEscape-v0),\n",
       " EnvSpec(gym-core.KangarooSync-v0),\n",
       " EnvSpec(gym-core.AmidarDeterministicSync-v3),\n",
       " EnvSpec(gym-core.AmidarDeterministicSync-v0),\n",
       " EnvSpec(Reacher-v1),\n",
       " EnvSpec(gym-core.BankHeistNoFrameskip-v0),\n",
       " EnvSpec(gym-core.BankHeistNoFrameskip-v3),\n",
       " EnvSpec(flashgames.FinalSiege-v0),\n",
       " EnvSpec(wob.real.Quizlet-Planet-Learn-v0),\n",
       " EnvSpec(flashgames.ClimbingSanta-v0),\n",
       " EnvSpec(gym-core.ChopperCommand-v0),\n",
       " EnvSpec(Tutankham-ramDeterministic-v4),\n",
       " EnvSpec(gym-core.ChopperCommand-v3),\n",
       " EnvSpec(Tutankham-ramDeterministic-v0),\n",
       " EnvSpec(flashgames.HeatRushFutureLvl4-v0),\n",
       " EnvSpec(gym-core.FrostbiteSlow-v0),\n",
       " EnvSpec(flashgames.CowboyVsUfos-v0),\n",
       " EnvSpec(gym-core.FrostbiteSlow-v3),\n",
       " EnvSpec(ElevatorAction-ramNoFrameskip-v0),\n",
       " EnvSpec(ElevatorAction-ramNoFrameskip-v4),\n",
       " EnvSpec(flashgames.CoasterCars2Contact-v0),\n",
       " EnvSpec(flashgames.JungleEagle-v0),\n",
       " EnvSpec(flashgames.CoasterRacerLvl2-v0),\n",
       " EnvSpec(flashgames.HighSpeedChase-v0),\n",
       " EnvSpec(gym-core.FreewayDeterministicSync-v0),\n",
       " EnvSpec(flashgames.HungerHunter-v0),\n",
       " EnvSpec(gym-core.FreewayDeterministicSync-v3),\n",
       " EnvSpec(gym-core.StarGunnerDeterministicSync-v3),\n",
       " EnvSpec(Bowling-v0),\n",
       " EnvSpec(PrivateEye-ramNoFrameskip-v0),\n",
       " EnvSpec(flashgames.RunRunRan-v0),\n",
       " EnvSpec(Bowling-v4),\n",
       " EnvSpec(PrivateEye-ramNoFrameskip-v4),\n",
       " EnvSpec(PitfallNoFrameskip-v0),\n",
       " EnvSpec(wob.real.Signup-12-v0),\n",
       " EnvSpec(flashgames.DriftRunners2-v0),\n",
       " EnvSpec(PitfallNoFrameskip-v4),\n",
       " EnvSpec(wob.real.Signup-4-v0),\n",
       " EnvSpec(CentipedeNoFrameskip-v0),\n",
       " EnvSpec(flashgames.MedievalShark-v0),\n",
       " EnvSpec(CentipedeNoFrameskip-v4),\n",
       " EnvSpec(gym-core.MontezumaRevengeDeterministicSlow-v3),\n",
       " EnvSpec(gym-core.MontezumaRevengeDeterministicSlow-v0),\n",
       " EnvSpec(gym-core.WizardOfWorDeterministicSync-v0),\n",
       " EnvSpec(gym-core.WizardOfWorDeterministicSync-v3),\n",
       " EnvSpec(flashgames.DinoBubble-v0),\n",
       " EnvSpec(flashgames.BubbleMover-v0),\n",
       " EnvSpec(flashgames.CoasterRacerLvl3-v0),\n",
       " EnvSpec(Riverraid-ramNoFrameskip-v4),\n",
       " EnvSpec(gym-core.DemonAttackDeterministic-v3),\n",
       " EnvSpec(Riverraid-ramNoFrameskip-v0),\n",
       " EnvSpec(wob.MiniWorldOfBits-v0),\n",
       " EnvSpec(wob.real.Signup-2-v0),\n",
       " EnvSpec(Alien-ramNoFrameskip-v4),\n",
       " EnvSpec(PrivateEye-v0),\n",
       " EnvSpec(flashgames.RedBeard-v0),\n",
       " EnvSpec(FrozenLake8x8-v0),\n",
       " EnvSpec(Alien-ramNoFrameskip-v0),\n",
       " EnvSpec(PrivateEye-v4),\n",
       " EnvSpec(gym-core.CartPoleLowDSync-v0),\n",
       " EnvSpec(flashgames.DnaLabRush-v0),\n",
       " EnvSpec(gym-core.Berzerk30FPS-v0),\n",
       " EnvSpec(gym-core.Berzerk30FPS-v3),\n",
       " EnvSpec(LunarLanderContinuous-v2),\n",
       " EnvSpec(flashgames.HeatRushFutureLvl12-v0),\n",
       " EnvSpec(Asterix-ram-v0),\n",
       " EnvSpec(flashgames.NinjaPainter-v0),\n",
       " EnvSpec(Asterix-ram-v4),\n",
       " EnvSpec(flashgames.GSwitch-v0),\n",
       " EnvSpec(flashgames.30Seconds-v0),\n",
       " EnvSpec(wob.real.Quizlet-Geography-Learn-v0),\n",
       " EnvSpec(wob.real.Quizlet-Comet-Test-v0),\n",
       " EnvSpec(QbertDeterministic-v0),\n",
       " EnvSpec(QbertDeterministic-v4),\n",
       " EnvSpec(flashgames.SpinSprint-v0),\n",
       " EnvSpec(flashgames.SmileyPuzzle-v0),\n",
       " EnvSpec(SolarisDeterministic-v0),\n",
       " EnvSpec(gym-core.Riverraid-v0),\n",
       " EnvSpec(gym-core.TennisNoFrameskip-v3),\n",
       " EnvSpec(SolarisDeterministic-v4),\n",
       " EnvSpec(gym-core.TennisNoFrameskip-v0),\n",
       " EnvSpec(flashgames.DriftRunners3d-v0),\n",
       " EnvSpec(JamesbondNoFrameskip-v4),\n",
       " EnvSpec(flashgames.BugsGotGuns-v0),\n",
       " EnvSpec(TwoRoundDeterministicReward-v0),\n",
       " EnvSpec(flashgames.NeonRace2-v0),\n",
       " EnvSpec(gym-core.StarGunnerDeterministicSlow-v0),\n",
       " EnvSpec(flashgames.BunnyCannon-v0),\n",
       " EnvSpec(gym-core.Frostbite-v0),\n",
       " EnvSpec(gym-core.Frostbite-v3),\n",
       " EnvSpec(PhoenixDeterministic-v4),\n",
       " EnvSpec(flashgames.Mrbirdie-v0),\n",
       " EnvSpec(PhoenixDeterministic-v0),\n",
       " EnvSpec(StarGunner-ramDeterministic-v4),\n",
       " EnvSpec(PredictActionsCartpole-v0),\n",
       " EnvSpec(flashgames.FlashDrive-v0),\n",
       " EnvSpec(StarGunner-ramDeterministic-v0),\n",
       " EnvSpec(BeamRider-ramDeterministic-v4),\n",
       " EnvSpec(BeamRider-ramDeterministic-v0),\n",
       " EnvSpec(flashgames.SmashTheSwine-v0),\n",
       " EnvSpec(gym-core.SpaceInvadersDeterministic-v3),\n",
       " EnvSpec(gym-core.SpaceInvadersDeterministic-v0),\n",
       " EnvSpec(flashgames.CemeteryRoad-v0),\n",
       " EnvSpec(flashgames.EvolutionRacingLvl5-v0),\n",
       " EnvSpec(wob.mini.ClickTab2-v0),\n",
       " EnvSpec(gym-core.BoxingDeterministicSync-v3),\n",
       " EnvSpec(flashgames.JollySwipe-v0),\n",
       " EnvSpec(gym-core.KangarooDeterministicSlow-v3),\n",
       " EnvSpec(NameThisGame-ramNoFrameskip-v4),\n",
       " EnvSpec(wob.mini.UseColorwheel-v0),\n",
       " EnvSpec(PongDeterministic-v4),\n",
       " EnvSpec(Pong-ramNoFrameskip-v0),\n",
       " EnvSpec(Pong-ramDeterministic-v0),\n",
       " EnvSpec(flashgames.Devilment-v0),\n",
       " EnvSpec(Pong-ramDeterministic-v4),\n",
       " EnvSpec(wob.mini.HighlightText2-v0),\n",
       " EnvSpec(gym-core.BankHeist30FPS-v3),\n",
       " EnvSpec(gym-core.BankHeist30FPS-v0),\n",
       " EnvSpec(gym-core.NameThisGameNoFrameskip-v3),\n",
       " EnvSpec(NameThisGame-ramNoFrameskip-v0),\n",
       " EnvSpec(gym-core.NameThisGameNoFrameskip-v0),\n",
       " EnvSpec(gym-core.PongSlow-v0),\n",
       " EnvSpec(gym-core.PongSlow-v3),\n",
       " EnvSpec(flashgames.ProjectMonochrome-v0),\n",
       " EnvSpec(gym-core.PooyanSlow-v3),\n",
       " EnvSpec(TutankhamNoFrameskip-v4),\n",
       " EnvSpec(flashgames.VirtualRacer-v0),\n",
       " EnvSpec(TutankhamNoFrameskip-v0),\n",
       " EnvSpec(flashgames.DotGrowth-v0),\n",
       " EnvSpec(flashgames.VanguardWars-v0),\n",
       " EnvSpec(gym-core.UpNDownSlow-v0),\n",
       " EnvSpec(wob.mini.EnterPassword-v0),\n",
       " EnvSpec(flashgames.MiniMachines-v0),\n",
       " EnvSpec(wob.mini.Terminal-v0),\n",
       " EnvSpec(gym-core.JourneyEscapeDeterministicSlow-v3),\n",
       " EnvSpec(gym-core.JourneyEscapeDeterministicSlow-v0),\n",
       " EnvSpec(flashgames.EvolutionRacingLvl16-v0),\n",
       " ...]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gym.envs.registry.all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running an Environment, Training and simulating(more below)\n",
    "    1. L(Load)  is set to False because I have no training data for that particular game.\n",
    "    Once trained, a training data is provided, then set L to True.\n",
    "    2. Set SO to True so that it can accumualative learn among all workers.\n",
    "### Interrupt the Kernal to Stop training or stop testing.\n",
    "    \n",
    "## Note: Important to run all cells above but don't run everything below this\n",
    "### The cells below are in sections, choose 1 section to run. E.g if you want to train, just run the Training section. or if you want to play pacman, just run the cells in the PacMan Section ( From input to render)\n",
    "    \n",
    "### Training Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*It is important to limit number of worker threads to number of cpu cores available\n",
    "More than one thread per cpu core available is detrimental in training speed and effectiveness*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "args = {'LR': 0.0001, \"G\":0.99, \"T\":1.00,\"W\":7,\"NS\":100,\"M\":10000,\"ENV\":'MsPacman-v0',\n",
    "         \"EC\":'./config.json',\"SO\":True,\"L\":False,\"SSL\":20, \"OPT\":\"Adam\",\"CL\":False,\n",
    "         \"LMD\":'./trained_models/',\"SMD\":\"./trained_models/\",\"LG\":'./logs/', \"seed\":42,\"config\":\"Default\"\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "undo_logger_setup()\n",
    "\n",
    "torch.set_default_tensor_type('torch.FloatTensor')\n",
    "torch.manual_seed(args['seed'])\n",
    "\n",
    "setup_json = read_config(args['EC'])\n",
    "env_conf = setup_json[args['config']]\n",
    "\n",
    "for i in setup_json.keys():\n",
    "    if i in args['ENV']:\n",
    "        env_conf = setup_json[i]\n",
    "env = atari_env(args['ENV'], env_conf)\n",
    "\n",
    "shared_model = A3Clstm(env.observation_space.shape[0], env.action_space)\n",
    "if args['L']:\n",
    "    saved_state = torch.load(\n",
    "        '{0}{1}.dat'.format(args['LMD'], args['ENV']))\n",
    "    shared_model.load_state_dict(saved_state)\n",
    "shared_model.share_memory()\n",
    "\n",
    "\n",
    "\n",
    "if args['SO']:\n",
    "    if args['OPT'] == 'RMSprop':\n",
    "        optimizer = SharedRMSprop(shared_model.parameters(), lr=args['LR'])\n",
    "    if args['OPT'] == 'Adam':\n",
    "        optimizer = SharedAdam(shared_model.parameters(), lr=args['LR'])\n",
    "    if args['OPT'] == 'LrSchedAdam':\n",
    "        optimizer = SharedLrSchedAdam(\n",
    "            shared_model.parameters(), lr=args['LR'])\n",
    "    optimizer.share_memory()\n",
    "else:\n",
    "    optimizer = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run This to Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-10-07 14:38:01,660 : OPT: Adam\n",
      "2017-10-07 14:38:01,660 : OPT: Adam\n",
      "2017-10-07 14:38:01,660 : OPT: Adam\n",
      "2017-10-07 14:38:01,660 : OPT: Adam\n",
      "2017-10-07 14:38:01,660 : OPT: Adam\n",
      "2017-10-07 14:38:01,660 : OPT: Adam\n",
      "2017-10-07 14:38:01,660 : OPT: Adam\n",
      "2017-10-07 14:38:01,704 : LG: ./logs/\n",
      "2017-10-07 14:38:01,704 : LG: ./logs/\n",
      "2017-10-07 14:38:01,704 : LG: ./logs/\n",
      "2017-10-07 14:38:01,704 : LG: ./logs/\n",
      "2017-10-07 14:38:01,704 : LG: ./logs/\n",
      "2017-10-07 14:38:01,704 : LG: ./logs/\n",
      "2017-10-07 14:38:01,704 : LG: ./logs/\n",
      "2017-10-07 14:38:01,709 : SMD: ./trained_models/\n",
      "2017-10-07 14:38:01,709 : SMD: ./trained_models/\n",
      "2017-10-07 14:38:01,709 : SMD: ./trained_models/\n",
      "2017-10-07 14:38:01,709 : SMD: ./trained_models/\n",
      "2017-10-07 14:38:01,709 : SMD: ./trained_models/\n",
      "2017-10-07 14:38:01,709 : SMD: ./trained_models/\n",
      "2017-10-07 14:38:01,709 : SMD: ./trained_models/\n",
      "2017-10-07 14:38:01,713 : ENV: MsPacman-v0\n",
      "2017-10-07 14:38:01,713 : ENV: MsPacman-v0\n",
      "2017-10-07 14:38:01,713 : ENV: MsPacman-v0\n",
      "2017-10-07 14:38:01,713 : ENV: MsPacman-v0\n",
      "2017-10-07 14:38:01,713 : ENV: MsPacman-v0\n",
      "2017-10-07 14:38:01,713 : ENV: MsPacman-v0\n",
      "2017-10-07 14:38:01,713 : ENV: MsPacman-v0\n",
      "2017-10-07 14:38:01,718 : G: 0.99\n",
      "2017-10-07 14:38:01,718 : G: 0.99\n",
      "2017-10-07 14:38:01,718 : G: 0.99\n",
      "2017-10-07 14:38:01,718 : G: 0.99\n",
      "2017-10-07 14:38:01,718 : G: 0.99\n",
      "2017-10-07 14:38:01,718 : G: 0.99\n",
      "2017-10-07 14:38:01,718 : G: 0.99\n",
      "2017-10-07 14:38:01,722 : CL: False\n",
      "2017-10-07 14:38:01,722 : CL: False\n",
      "2017-10-07 14:38:01,722 : CL: False\n",
      "2017-10-07 14:38:01,722 : CL: False\n",
      "2017-10-07 14:38:01,722 : CL: False\n",
      "2017-10-07 14:38:01,722 : CL: False\n",
      "2017-10-07 14:38:01,722 : CL: False\n",
      "2017-10-07 14:38:01,734 : config: Default\n",
      "2017-10-07 14:38:01,734 : config: Default\n",
      "2017-10-07 14:38:01,734 : config: Default\n",
      "2017-10-07 14:38:01,734 : config: Default\n",
      "2017-10-07 14:38:01,734 : config: Default\n",
      "2017-10-07 14:38:01,734 : config: Default\n",
      "2017-10-07 14:38:01,734 : config: Default\n",
      "2017-10-07 14:38:01,740 : M: 10000\n",
      "2017-10-07 14:38:01,740 : M: 10000\n",
      "2017-10-07 14:38:01,740 : M: 10000\n",
      "2017-10-07 14:38:01,740 : M: 10000\n",
      "2017-10-07 14:38:01,740 : M: 10000\n",
      "2017-10-07 14:38:01,740 : M: 10000\n",
      "2017-10-07 14:38:01,740 : M: 10000\n",
      "2017-10-07 14:38:01,745 : L: False\n",
      "2017-10-07 14:38:01,745 : L: False\n",
      "2017-10-07 14:38:01,745 : L: False\n",
      "2017-10-07 14:38:01,745 : L: False\n",
      "2017-10-07 14:38:01,745 : L: False\n",
      "2017-10-07 14:38:01,745 : L: False\n",
      "2017-10-07 14:38:01,745 : L: False\n",
      "2017-10-07 14:38:01,750 : EC: ./config.json\n",
      "2017-10-07 14:38:01,750 : EC: ./config.json\n",
      "2017-10-07 14:38:01,750 : EC: ./config.json\n",
      "2017-10-07 14:38:01,750 : EC: ./config.json\n",
      "2017-10-07 14:38:01,750 : EC: ./config.json\n",
      "2017-10-07 14:38:01,750 : EC: ./config.json\n",
      "2017-10-07 14:38:01,750 : EC: ./config.json\n",
      "2017-10-07 14:38:01,768 : SSL: 20\n",
      "2017-10-07 14:38:01,768 : SSL: 20\n",
      "2017-10-07 14:38:01,768 : SSL: 20\n",
      "2017-10-07 14:38:01,768 : SSL: 20\n",
      "2017-10-07 14:38:01,768 : SSL: 20\n",
      "2017-10-07 14:38:01,768 : SSL: 20\n",
      "2017-10-07 14:38:01,768 : SSL: 20\n",
      "2017-10-07 14:38:01,781 : seed: 42\n",
      "2017-10-07 14:38:01,781 : seed: 42\n",
      "2017-10-07 14:38:01,781 : seed: 42\n",
      "2017-10-07 14:38:01,781 : seed: 42\n",
      "2017-10-07 14:38:01,781 : seed: 42\n",
      "2017-10-07 14:38:01,781 : seed: 42\n",
      "2017-10-07 14:38:01,781 : seed: 42\n",
      "2017-10-07 14:38:01,789 : LR: 0.0001\n",
      "2017-10-07 14:38:01,789 : LR: 0.0001\n",
      "2017-10-07 14:38:01,789 : LR: 0.0001\n",
      "2017-10-07 14:38:01,789 : LR: 0.0001\n",
      "2017-10-07 14:38:01,789 : LR: 0.0001\n",
      "2017-10-07 14:38:01,789 : LR: 0.0001\n",
      "2017-10-07 14:38:01,789 : LR: 0.0001\n",
      "2017-10-07 14:38:01,795 : T: 1.0\n",
      "2017-10-07 14:38:01,795 : T: 1.0\n",
      "2017-10-07 14:38:01,795 : T: 1.0\n",
      "2017-10-07 14:38:01,795 : T: 1.0\n",
      "2017-10-07 14:38:01,795 : T: 1.0\n",
      "2017-10-07 14:38:01,795 : T: 1.0\n",
      "2017-10-07 14:38:01,795 : T: 1.0\n",
      "2017-10-07 14:38:01,810 : W: 7\n",
      "2017-10-07 14:38:01,810 : W: 7\n",
      "2017-10-07 14:38:01,810 : W: 7\n",
      "2017-10-07 14:38:01,810 : W: 7\n",
      "2017-10-07 14:38:01,810 : W: 7\n",
      "2017-10-07 14:38:01,810 : W: 7\n",
      "2017-10-07 14:38:01,810 : W: 7\n",
      "2017-10-07 14:38:01,816 : SO: True\n",
      "2017-10-07 14:38:01,816 : SO: True\n",
      "2017-10-07 14:38:01,816 : SO: True\n",
      "2017-10-07 14:38:01,816 : SO: True\n",
      "2017-10-07 14:38:01,816 : SO: True\n",
      "2017-10-07 14:38:01,816 : SO: True\n",
      "2017-10-07 14:38:01,816 : SO: True\n",
      "2017-10-07 14:38:01,827 : NS: 100\n",
      "2017-10-07 14:38:01,827 : NS: 100\n",
      "2017-10-07 14:38:01,827 : NS: 100\n",
      "2017-10-07 14:38:01,827 : NS: 100\n",
      "2017-10-07 14:38:01,827 : NS: 100\n",
      "2017-10-07 14:38:01,827 : NS: 100\n",
      "2017-10-07 14:38:01,827 : NS: 100\n",
      "2017-10-07 14:38:01,833 : LMD: ./trained_models/\n",
      "2017-10-07 14:38:01,833 : LMD: ./trained_models/\n",
      "2017-10-07 14:38:01,833 : LMD: ./trained_models/\n",
      "2017-10-07 14:38:01,833 : LMD: ./trained_models/\n",
      "2017-10-07 14:38:01,833 : LMD: ./trained_models/\n",
      "2017-10-07 14:38:01,833 : LMD: ./trained_models/\n",
      "2017-10-07 14:38:01,833 : LMD: ./trained_models/\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-b79748cd9415>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprocesses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/nasdin/anaconda2/lib/python2.7/multiprocessing/process.pyc\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_pid\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'can only join a child process'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'can only join a started process'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m             \u001b[0m_current_process\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_children\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/nasdin/anaconda2/lib/python2.7/multiprocessing/forking.pyc\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m             \u001b[0mdeadline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m             \u001b[0mdelay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0005\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/nasdin/anaconda2/lib/python2.7/multiprocessing/forking.pyc\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, flag)\u001b[0m\n\u001b[1;32m    133\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m                     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m                         \u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwaitpid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrno\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0merrno\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEINTR\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "processes = []\n",
    "\n",
    "p = Process(target=test, args=(args, shared_model, env_conf))\n",
    "p.start()\n",
    "processes.append(p)\n",
    "time.sleep(0.1)\n",
    "for rank in range(0, args['W']):\n",
    "    p = Process(\n",
    "        target=train, args=(rank, args, shared_model, optimizer, env_conf))\n",
    "    p.start()\n",
    "    processes.append(p)\n",
    "    time.sleep(0.1)\n",
    "for p in processes:\n",
    "    time.sleep(0.1)\n",
    "    p.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playing the Atari Games Section\n",
    "## The best part\n",
    "### WEEEEEEEEEEEEe\n",
    "#### If it gives tensorflow errors, just run the cells again and somehow it works the 2nd time. This is because we don't have the full Share optimizer data generated yet\n",
    "### Load model is disabled on default so that you can observe how it learns through iteration,  Set L to True if you want to load the trained models and see how well it performs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playing PacMan (10000 episodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "args = {'LR': 0.0001, \"G\":0.99, \"T\":1.00,\"W\":7,\"NS\":20,\"M\":10000,\"ENV\":'MsPacman-v0',\n",
    "         \"EC\":'./config.json',\"SO\":True,\"L\":False,\"SSL\":20, \"OPT\":\"Adam\",\"CL\":False,\n",
    "         \"LMD\":'./trained_models/',\"SMD\":\"./trained_models/\",\"LG\":'./logs/', \"seed\":42,\"config\":\"MsPacman\"\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "undo_logger_setup()\n",
    "\n",
    "torch.set_default_tensor_type('torch.FloatTensor')\n",
    "torch.manual_seed(args['seed'])\n",
    "\n",
    "setup_json = read_config(args['EC'])\n",
    "env_conf = setup_json[args['config']]\n",
    "\n",
    "for i in setup_json.keys():\n",
    "    if i in args['ENV']:\n",
    "        env_conf = setup_json[i]\n",
    "env = atari_env(args['ENV'], env_conf)\n",
    "\n",
    "shared_model = A3Clstm(env.observation_space.shape[0], env.action_space)\n",
    "if args['L']:\n",
    "    saved_state = torch.load(\n",
    "        '{0}{1}.dat'.format(args['LMD'], args['ENV']))\n",
    "    shared_model.load_state_dict(saved_state)\n",
    "shared_model.share_memory()\n",
    "\n",
    "\n",
    "\n",
    "if args['SO']:\n",
    "    if args['OPT'] == 'RMSprop':\n",
    "        optimizer = SharedRMSprop(shared_model.parameters(), lr=args['LR'])\n",
    "    if args['OPT'] == 'Adam':\n",
    "        optimizer = SharedAdam(shared_model.parameters(), lr=args['LR'])\n",
    "    if args['OPT'] == 'LrSchedAdam':\n",
    "        optimizer = SharedLrSchedAdam(\n",
    "            shared_model.parameters(), lr=args['LR'])\n",
    "    optimizer.share_memory()\n",
    "else:\n",
    "    optimizer = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run this to Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-10-07 14:37:26,808 : OPT: Adam\n",
      "2017-10-07 14:37:26,808 : OPT: Adam\n",
      "2017-10-07 14:37:26,808 : OPT: Adam\n",
      "2017-10-07 14:37:26,808 : OPT: Adam\n",
      "2017-10-07 14:37:26,808 : OPT: Adam\n",
      "2017-10-07 14:37:26,808 : OPT: Adam\n",
      "2017-10-07 14:37:26,850 : LG: ./logs/\n",
      "2017-10-07 14:37:26,850 : LG: ./logs/\n",
      "2017-10-07 14:37:26,850 : LG: ./logs/\n",
      "2017-10-07 14:37:26,850 : LG: ./logs/\n",
      "2017-10-07 14:37:26,850 : LG: ./logs/\n",
      "2017-10-07 14:37:26,850 : LG: ./logs/\n",
      "2017-10-07 14:37:26,856 : SMD: ./trained_models/\n",
      "2017-10-07 14:37:26,856 : SMD: ./trained_models/\n",
      "2017-10-07 14:37:26,856 : SMD: ./trained_models/\n",
      "2017-10-07 14:37:26,856 : SMD: ./trained_models/\n",
      "2017-10-07 14:37:26,856 : SMD: ./trained_models/\n",
      "2017-10-07 14:37:26,856 : SMD: ./trained_models/\n",
      "2017-10-07 14:37:26,861 : ENV: MsPacman-v0\n",
      "2017-10-07 14:37:26,861 : ENV: MsPacman-v0\n",
      "2017-10-07 14:37:26,861 : ENV: MsPacman-v0\n",
      "2017-10-07 14:37:26,861 : ENV: MsPacman-v0\n",
      "2017-10-07 14:37:26,861 : ENV: MsPacman-v0\n",
      "2017-10-07 14:37:26,861 : ENV: MsPacman-v0\n",
      "2017-10-07 14:37:26,865 : G: 0.99\n",
      "2017-10-07 14:37:26,865 : G: 0.99\n",
      "2017-10-07 14:37:26,865 : G: 0.99\n",
      "2017-10-07 14:37:26,865 : G: 0.99\n",
      "2017-10-07 14:37:26,865 : G: 0.99\n",
      "2017-10-07 14:37:26,865 : G: 0.99\n",
      "2017-10-07 14:37:26,868 : CL: False\n",
      "2017-10-07 14:37:26,868 : CL: False\n",
      "2017-10-07 14:37:26,868 : CL: False\n",
      "2017-10-07 14:37:26,868 : CL: False\n",
      "2017-10-07 14:37:26,868 : CL: False\n",
      "2017-10-07 14:37:26,868 : CL: False\n",
      "2017-10-07 14:37:26,871 : config: MsPacman\n",
      "2017-10-07 14:37:26,871 : config: MsPacman\n",
      "2017-10-07 14:37:26,871 : config: MsPacman\n",
      "2017-10-07 14:37:26,871 : config: MsPacman\n",
      "2017-10-07 14:37:26,871 : config: MsPacman\n",
      "2017-10-07 14:37:26,871 : config: MsPacman\n",
      "2017-10-07 14:37:26,877 : M: 10000\n",
      "2017-10-07 14:37:26,877 : M: 10000\n",
      "2017-10-07 14:37:26,877 : M: 10000\n",
      "2017-10-07 14:37:26,877 : M: 10000\n",
      "2017-10-07 14:37:26,877 : M: 10000\n",
      "2017-10-07 14:37:26,877 : M: 10000\n",
      "2017-10-07 14:37:26,880 : L: False\n",
      "2017-10-07 14:37:26,880 : L: False\n",
      "2017-10-07 14:37:26,880 : L: False\n",
      "2017-10-07 14:37:26,880 : L: False\n",
      "2017-10-07 14:37:26,880 : L: False\n",
      "2017-10-07 14:37:26,880 : L: False\n",
      "2017-10-07 14:37:26,884 : EC: ./config.json\n",
      "2017-10-07 14:37:26,884 : EC: ./config.json\n",
      "2017-10-07 14:37:26,884 : EC: ./config.json\n",
      "2017-10-07 14:37:26,884 : EC: ./config.json\n",
      "2017-10-07 14:37:26,884 : EC: ./config.json\n",
      "2017-10-07 14:37:26,884 : EC: ./config.json\n",
      "2017-10-07 14:37:26,889 : SSL: 20\n",
      "2017-10-07 14:37:26,889 : SSL: 20\n",
      "2017-10-07 14:37:26,889 : SSL: 20\n",
      "2017-10-07 14:37:26,889 : SSL: 20\n",
      "2017-10-07 14:37:26,889 : SSL: 20\n",
      "2017-10-07 14:37:26,889 : SSL: 20\n",
      "2017-10-07 14:37:26,894 : seed: 42\n",
      "2017-10-07 14:37:26,894 : seed: 42\n",
      "2017-10-07 14:37:26,894 : seed: 42\n",
      "2017-10-07 14:37:26,894 : seed: 42\n",
      "2017-10-07 14:37:26,894 : seed: 42\n",
      "2017-10-07 14:37:26,894 : seed: 42\n",
      "2017-10-07 14:37:26,899 : LR: 0.0001\n",
      "2017-10-07 14:37:26,899 : LR: 0.0001\n",
      "2017-10-07 14:37:26,899 : LR: 0.0001\n",
      "2017-10-07 14:37:26,899 : LR: 0.0001\n",
      "2017-10-07 14:37:26,899 : LR: 0.0001\n",
      "2017-10-07 14:37:26,899 : LR: 0.0001\n",
      "2017-10-07 14:37:26,903 : T: 1.0\n",
      "2017-10-07 14:37:26,903 : T: 1.0\n",
      "2017-10-07 14:37:26,903 : T: 1.0\n",
      "2017-10-07 14:37:26,903 : T: 1.0\n",
      "2017-10-07 14:37:26,903 : T: 1.0\n",
      "2017-10-07 14:37:26,903 : T: 1.0\n",
      "2017-10-07 14:37:26,909 : W: 7\n",
      "2017-10-07 14:37:26,909 : W: 7\n",
      "2017-10-07 14:37:26,909 : W: 7\n",
      "2017-10-07 14:37:26,909 : W: 7\n",
      "2017-10-07 14:37:26,909 : W: 7\n",
      "2017-10-07 14:37:26,909 : W: 7\n",
      "2017-10-07 14:37:26,915 : SO: True\n",
      "2017-10-07 14:37:26,915 : SO: True\n",
      "2017-10-07 14:37:26,915 : SO: True\n",
      "2017-10-07 14:37:26,915 : SO: True\n",
      "2017-10-07 14:37:26,915 : SO: True\n",
      "2017-10-07 14:37:26,915 : SO: True\n",
      "2017-10-07 14:37:26,920 : NS: 100\n",
      "2017-10-07 14:37:26,920 : NS: 100\n",
      "2017-10-07 14:37:26,920 : NS: 100\n",
      "2017-10-07 14:37:26,920 : NS: 100\n",
      "2017-10-07 14:37:26,920 : NS: 100\n",
      "2017-10-07 14:37:26,920 : NS: 100\n",
      "2017-10-07 14:37:26,929 : LMD: ./trained_models/\n",
      "2017-10-07 14:37:26,929 : LMD: ./trained_models/\n",
      "2017-10-07 14:37:26,929 : LMD: ./trained_models/\n",
      "2017-10-07 14:37:26,929 : LMD: ./trained_models/\n",
      "2017-10-07 14:37:26,929 : LMD: ./trained_models/\n",
      "2017-10-07 14:37:26,929 : LMD: ./trained_models/\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-40b60a3004a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshared_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv_conf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-19-12a1f9002f83>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(args, shared_model, env_conf, render)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mplayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mreward_sum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mplayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-04b357b58ab4>\u001b[0m in \u001b[0;36maction_test\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvolatile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvolatile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvolatile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0mprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/nasdin/anaconda2/lib/python2.7/site-packages/torch/nn/modules/module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-bc8f4f957771>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxp1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxp2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxp3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/nasdin/anaconda2/lib/python2.7/site-packages/torch/nn/modules/module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/nasdin/anaconda2/lib/python2.7/site-packages/torch/nn/modules/pooling.pyc\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    141\u001b[0m         return F.max_pool2d(input, self.kernel_size, self.stride,\n\u001b[1;32m    142\u001b[0m                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil_mode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m                             self.return_indices)\n\u001b[0m\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/nasdin/anaconda2/lib/python2.7/site-packages/torch/nn/functional.pyc\u001b[0m in \u001b[0;36mmax_pool2d\u001b[0;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[0m\n\u001b[1;32m    284\u001b[0m                ceil_mode=False, return_indices=False):\n\u001b[1;32m    285\u001b[0m     ret = _functions.thnn.MaxPool2d.apply(input, kernel_size, stride, padding, dilation,\n\u001b[0;32m--> 286\u001b[0;31m                                           ceil_mode)\n\u001b[0m\u001b[1;32m    287\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_indices\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/nasdin/anaconda2/lib/python2.7/site-packages/torch/nn/_functions/thnn/pooling.pyc\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, input, kernel_size, stride, padding, dilation, ceil_mode)\u001b[0m\n\u001b[1;32m     96\u001b[0m                                                       \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m                                                       \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m                                                       ctx.ceil_mode)\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_for_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmark_non_differentiable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test(args, shared_model, env_conf,render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playing BeamRider (4000 episodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "args = {'LR': 0.0001, \"G\":0.99, \"T\":1.00,\"W\":7,\"NS\":20,\"M\":4000,\"ENV\":'BeamRider-v0',\n",
    "         \"EC\":'./config.json',\"SO\":True,\"L\":False,\"SSL\":20, \"OPT\":\"Adam\",\"CL\":False,\n",
    "         \"LMD\":'./trained_models/',\"SMD\":\"./trained_models/\",\"LG\":'./logs/', \"seed\":42,\"config\":\"BeamRider\"\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "undo_logger_setup()\n",
    "\n",
    "torch.set_default_tensor_type('torch.FloatTensor')\n",
    "torch.manual_seed(args['seed'])\n",
    "\n",
    "setup_json = read_config(args['EC'])\n",
    "env_conf = setup_json[args['config']]\n",
    "\n",
    "for i in setup_json.keys():\n",
    "    if i in args['ENV']:\n",
    "        env_conf = setup_json[i]\n",
    "env = atari_env(args['ENV'], env_conf)\n",
    "\n",
    "shared_model = A3Clstm(env.observation_space.shape[0], env.action_space)\n",
    "if args['L']:\n",
    "    saved_state = torch.load(\n",
    "        '{0}{1}.dat'.format(args['LMD'], args['ENV']))\n",
    "    shared_model.load_state_dict(saved_state)\n",
    "shared_model.share_memory()\n",
    "\n",
    "\n",
    "\n",
    "if args['SO']:\n",
    "    if args['OPT'] == 'RMSprop':\n",
    "        optimizer = SharedRMSprop(shared_model.parameters(), lr=args['LR'])\n",
    "    if args['OPT'] == 'Adam':\n",
    "        optimizer = SharedAdam(shared_model.parameters(), lr=args['LR'])\n",
    "    if args['OPT'] == 'LrSchedAdam':\n",
    "        optimizer = SharedLrSchedAdam(\n",
    "            shared_model.parameters(), lr=args['LR'])\n",
    "    optimizer.share_memory()\n",
    "else:\n",
    "    optimizer = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run this to Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-10-07 14:32:34,412 : OPT: Adam\n",
      "2017-10-07 14:32:34,412 : OPT: Adam\n",
      "2017-10-07 14:32:34,449 : LG: ./logs/\n",
      "2017-10-07 14:32:34,449 : LG: ./logs/\n",
      "2017-10-07 14:32:34,451 : SMD: ./trained_models/\n",
      "2017-10-07 14:32:34,451 : SMD: ./trained_models/\n",
      "2017-10-07 14:32:34,453 : ENV: BeamRider-v0\n",
      "2017-10-07 14:32:34,453 : ENV: BeamRider-v0\n",
      "2017-10-07 14:32:34,454 : G: 0.99\n",
      "2017-10-07 14:32:34,454 : G: 0.99\n",
      "2017-10-07 14:32:34,456 : CL: False\n",
      "2017-10-07 14:32:34,456 : CL: False\n",
      "2017-10-07 14:32:34,458 : config: BeamRider\n",
      "2017-10-07 14:32:34,458 : config: BeamRider\n",
      "2017-10-07 14:32:34,460 : M: 4000\n",
      "2017-10-07 14:32:34,460 : M: 4000\n",
      "2017-10-07 14:32:34,461 : L: False\n",
      "2017-10-07 14:32:34,461 : L: False\n",
      "2017-10-07 14:32:34,463 : EC: ./config.json\n",
      "2017-10-07 14:32:34,463 : EC: ./config.json\n",
      "2017-10-07 14:32:34,465 : SSL: 20\n",
      "2017-10-07 14:32:34,465 : SSL: 20\n",
      "2017-10-07 14:32:34,467 : seed: 42\n",
      "2017-10-07 14:32:34,467 : seed: 42\n",
      "2017-10-07 14:32:34,468 : LR: 0.0001\n",
      "2017-10-07 14:32:34,468 : LR: 0.0001\n",
      "2017-10-07 14:32:34,470 : T: 1.0\n",
      "2017-10-07 14:32:34,470 : T: 1.0\n",
      "2017-10-07 14:32:34,471 : W: 7\n",
      "2017-10-07 14:32:34,471 : W: 7\n",
      "2017-10-07 14:32:34,473 : SO: True\n",
      "2017-10-07 14:32:34,473 : SO: True\n",
      "2017-10-07 14:32:34,474 : NS: 20\n",
      "2017-10-07 14:32:34,474 : NS: 20\n",
      "2017-10-07 14:32:34,475 : LMD: ./trained_models/\n",
      "2017-10-07 14:32:34,475 : LMD: ./trained_models/\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-40b60a3004a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshared_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv_conf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-19-12a1f9002f83>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(args, shared_model, env_conf, render)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mplayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mreward_sum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mplayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-04b357b58ab4>\u001b[0m in \u001b[0;36maction_test\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meps_len\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/nasdin/anaconda2/lib/python2.7/site-packages/gym/core.pyc\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0minfo\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcontains\u001b[0m \u001b[0mauxiliary\u001b[0m \u001b[0mdiagnostic\u001b[0m \u001b[0minformation\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhelpful\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdebugging\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msometimes\u001b[0m \u001b[0mlearning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \"\"\"\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/nasdin/HDD/GitHub/universe/universe/wrappers/vectorize.pyc\u001b[0m in \u001b[0;36m_step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0maction_n\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mobservation_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobservation_n\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward_n\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone_n\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'n'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/nasdin/anaconda2/lib/python2.7/site-packages/gym/core.pyc\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0minfo\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcontains\u001b[0m \u001b[0mauxiliary\u001b[0m \u001b[0mdiagnostic\u001b[0m \u001b[0minformation\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhelpful\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdebugging\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msometimes\u001b[0m \u001b[0mlearning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \"\"\"\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/nasdin/anaconda2/lib/python2.7/site-packages/gym/core.pyc\u001b[0m in \u001b[0;36m_step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/nasdin/anaconda2/lib/python2.7/site-packages/gym/core.pyc\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0minfo\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcontains\u001b[0m \u001b[0mauxiliary\u001b[0m \u001b[0mdiagnostic\u001b[0m \u001b[0minformation\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhelpful\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdebugging\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msometimes\u001b[0m \u001b[0mlearning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \"\"\"\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/nasdin/anaconda2/lib/python2.7/site-packages/gym/core.pyc\u001b[0m in \u001b[0;36m_step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    314\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/nasdin/anaconda2/lib/python2.7/site-packages/gym/core.pyc\u001b[0m in \u001b[0;36mobservation\u001b[0;34m(self, observation)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_observation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_observation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-8800a111976d>\u001b[0m in \u001b[0;36m_observation\u001b[0;34m(self, observation_n)\u001b[0m\n\u001b[1;32m      8\u001b[0m         return [\n\u001b[1;32m      9\u001b[0m             \u001b[0m_process_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobservation\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mobservation_n\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         ]\n",
      "\u001b[0;32m<ipython-input-11-c9f8e84d9f04>\u001b[0m in \u001b[0;36m_process_frame\u001b[0;34m(frame, conf)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_process_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"crop1\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"crop2\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m160\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;36m160\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrgb2gray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m80\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"dimension2\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m80\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m80\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m80\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m80\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test(args, shared_model, env_conf,render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playing Breakout (3000 episodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "args = {'LR': 0.0001, \"G\":0.99, \"T\":1.00, \"S\":1,\"W\":7,\"NS\":20,\"M\":3000,\"ENV\":'Breakout-v0',\n",
    "         \"EC\":'./config.json',\"SO\":True,\"L\":False,\"SSL\":20, \"OPT\":\"Adam\",\"CL\":False,\n",
    "         \"LMD\":'./trained_models/',\"SMD\":\"./trained_models/\",\"LG\":'./logs/', \"seed\":42,\"config\":\"Breakout\"\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "undo_logger_setup()\n",
    "\n",
    "torch.set_default_tensor_type('torch.FloatTensor')\n",
    "torch.manual_seed(args['seed'])\n",
    "\n",
    "setup_json = read_config(args['EC'])\n",
    "env_conf = setup_json[args['config']]\n",
    "\n",
    "for i in setup_json.keys():\n",
    "    if i in args['ENV']:\n",
    "        env_conf = setup_json[i]\n",
    "env = atari_env(args['ENV'], env_conf)\n",
    "\n",
    "shared_model = A3Clstm(env.observation_space.shape[0], env.action_space)\n",
    "if args['L']:\n",
    "    saved_state = torch.load(\n",
    "        '{0}{1}.dat'.format(args['LMD'], args['ENV']))\n",
    "    shared_model.load_state_dict(saved_state)\n",
    "shared_model.share_memory()\n",
    "\n",
    "\n",
    "\n",
    "if args['SO']:\n",
    "    if args['OPT'] == 'RMSprop':\n",
    "        optimizer = SharedRMSprop(shared_model.parameters(), lr=args['LR'])\n",
    "    if args['OPT'] == 'Adam':\n",
    "        optimizer = SharedAdam(shared_model.parameters(), lr=args['LR'])\n",
    "    if args['OPT'] == 'LrSchedAdam':\n",
    "        optimizer = SharedLrSchedAdam(\n",
    "            shared_model.parameters(), lr=args['LR'])\n",
    "    optimizer.share_memory()\n",
    "else:\n",
    "    optimizer = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run this to Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-10-07 14:32:50,550 : OPT: Adam\n",
      "2017-10-07 14:32:50,551 : LG: ./logs/\n",
      "2017-10-07 14:32:50,551 : SMD: ./trained_models/\n",
      "2017-10-07 14:32:50,552 : ENV: Breakout-v0\n",
      "2017-10-07 14:32:50,553 : G: 0.99\n",
      "2017-10-07 14:32:50,554 : CL: False\n",
      "2017-10-07 14:32:50,555 : config: Breakout\n",
      "2017-10-07 14:32:50,555 : M: 3000\n",
      "2017-10-07 14:32:50,556 : L: False\n",
      "2017-10-07 14:32:50,557 : EC: ./config.json\n",
      "2017-10-07 14:32:50,558 : SSL: 20\n",
      "2017-10-07 14:32:50,558 : S: 1\n",
      "2017-10-07 14:32:50,559 : seed: 42\n",
      "2017-10-07 14:32:50,560 : LR: 0.0001\n",
      "2017-10-07 14:32:50,561 : T: 1.0\n",
      "2017-10-07 14:32:50,561 : W: 7\n",
      "2017-10-07 14:32:50,562 : SO: True\n",
      "2017-10-07 14:32:50,563 : NS: 20\n",
      "2017-10-07 14:32:50,564 : LMD: ./trained_models/\n",
      "2017-10-07 14:33:00,461 : Time 00h 00m 09s, episode reward 0.0, episode length 163, reward mean 0.0000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-40b60a3004a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshared_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv_conf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-19-12a1f9002f83>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(args, shared_model, env_conf, render)\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mplayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meps_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m             \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m             \u001b[0mplayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test(args, shared_model, env_conf,render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playing SpaceInvader (10000 episodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "args = {'LR': 0.0001, \"G\":0.99, \"T\":1.00, \"S\":1,\"W\":7,\"NS\":20,\"M\":10000,\"ENV\":'SpaceInvaders-v0',\n",
    "         \"EC\":'./config.json',\"SO\":True,\"L\":False,\"SSL\":20, \"OPT\":\"Adam\",\"CL\":False,\n",
    "         \"LMD\":'./trained_models/',\"SMD\":\"./trained_models/\",\"LG\":'./logs/', \"seed\":42,\"config\":\"SpaceInvaders\"\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "undo_logger_setup()\n",
    "\n",
    "torch.set_default_tensor_type('torch.FloatTensor')\n",
    "torch.manual_seed(args['seed'])\n",
    "\n",
    "setup_json = read_config(args['EC'])\n",
    "env_conf = setup_json[args['config']]\n",
    "\n",
    "for i in setup_json.keys():\n",
    "    if i in args['ENV']:\n",
    "        env_conf = setup_json[i]\n",
    "env = atari_env(args['ENV'], env_conf)\n",
    "\n",
    "shared_model = A3Clstm(env.observation_space.shape[0], env.action_space)\n",
    "if args['L']:\n",
    "    saved_state = torch.load(\n",
    "        '{0}{1}.dat'.format(args['LMD'], args['ENV']))\n",
    "    shared_model.load_state_dict(saved_state)\n",
    "shared_model.share_memory()\n",
    "\n",
    "\n",
    "\n",
    "if args['SO']:\n",
    "    if args['OPT'] == 'RMSprop':\n",
    "        optimizer = SharedRMSprop(shared_model.parameters(), lr=args['LR'])\n",
    "    if args['OPT'] == 'Adam':\n",
    "        optimizer = SharedAdam(shared_model.parameters(), lr=args['LR'])\n",
    "    if args['OPT'] == 'LrSchedAdam':\n",
    "        optimizer = SharedLrSchedAdam(\n",
    "            shared_model.parameters(), lr=args['LR'])\n",
    "    optimizer.share_memory()\n",
    "else:\n",
    "    optimizer = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run this to Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test(args, shared_model, env_conf,render=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
