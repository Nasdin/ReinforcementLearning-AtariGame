{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open AI Atari Games. Reinforcement Learning with PyTorch, deep Learning\n",
    "## By Nasrudin Bin Salim\n",
    "### Requirements: Python 2.7. Linux Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import numpy as np\n",
    "\n",
    "import json\n",
    "import logging\n",
    "from cv2 import resize\n",
    "from skimage.color import rgb2gray\n",
    "import os\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "import argparse\n",
    "import math\n",
    "import time\n",
    "\n",
    "from torch.autograd import Variable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import OpenAI Universe environment and gym\n",
    "### Import Pytorch for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import universe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "from universe import vectorized\n",
    "from universe.wrappers import Unvectorize, Vectorize\n",
    "\n",
    "from gym.spaces.box import Box\n",
    "from gym.configuration import undo_logger_setup\n",
    "\n",
    "import torch\n",
    "from torch.multiprocessing import Process\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "#from skimage.transform import resize\n",
    "#from scipy.misc import imresize as resize\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions Section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logging function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def setup_logger(logger_name, log_file, level=logging.INFO):\n",
    "    l = logging.getLogger(logger_name)\n",
    "    formatter = logging.Formatter('%(asctime)s : %(message)s')\n",
    "    fileHandler = logging.FileHandler(log_file, mode='w')\n",
    "    fileHandler.setFormatter(formatter)\n",
    "    streamHandler = logging.StreamHandler()\n",
    "    streamHandler.setFormatter(formatter)\n",
    "\n",
    "    l.setLevel(level)\n",
    "    l.addHandler(fileHandler)\n",
    "    l.addHandler(streamHandler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Json Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_config(file_path):\n",
    "    \"\"\"Read JSON config.\"\"\"\n",
    "    json_object = json.load(open(file_path, 'r'))\n",
    "    return json_object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def norm_col_init(weights, std=1.0):\n",
    "    x = torch.randn(weights.size())\n",
    "    x *= std / torch.sqrt((x**2).sum(1, keepdim=True))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Share grads between 2 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ensure_shared_grads(model, shared_model):\n",
    "    for param, shared_param in zip(model.parameters(),\n",
    "                                   shared_model.parameters()):\n",
    "        if shared_param.grad is not None:\n",
    "            return\n",
    "        shared_param._grad = param.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        weight_shape = list(m.weight.data.size())\n",
    "        fan_in = np.prod(weight_shape[1:4])\n",
    "        fan_out = np.prod(weight_shape[2:4]) * weight_shape[0]\n",
    "        w_bound = np.sqrt(6. / (fan_in + fan_out))\n",
    "        m.weight.data.uniform_(-w_bound, w_bound)\n",
    "        m.bias.data.fill_(0)\n",
    "    elif classname.find('Linear') != -1:\n",
    "        weight_shape = list(m.weight.data.size())\n",
    "        fan_in = weight_shape[1]\n",
    "        fan_out = weight_shape[0]\n",
    "        w_bound = np.sqrt(6. / (fan_in + fan_out))\n",
    "        m.weight.data.uniform_(-w_bound, w_bound)\n",
    "        m.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the atari environment function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def atari_env(env_id, env_conf):\n",
    "    env = gym.make(env_id)\n",
    "    if len(env.observation_space.shape) > 1:\n",
    "        env = Vectorize(env)\n",
    "        env = AtariRescale(env, env_conf)\n",
    "        env = NormalizedEnv(env)\n",
    "        env = Unvectorize(env)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a frame for environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _process_frame(frame, conf):\n",
    "    frame = frame[conf[\"crop1\"]:conf[\"crop2\"] + 160, :160]\n",
    "    frame = resize(rgb2gray(frame), (80, conf[\"dimension2\"]))\n",
    "    frame = resize(frame, (80, 80))\n",
    "    frame = np.reshape(frame, [1, 80, 80])\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Atari rescale class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AtariRescale(vectorized.ObservationWrapper):\n",
    "    def __init__(self, env, env_conf):\n",
    "        super(AtariRescale, self).__init__(env)\n",
    "        self.observation_space = Box(0.0, 1.0, [1, 80, 80])\n",
    "        self.conf = env_conf\n",
    "\n",
    "    def _observation(self, observation_n):\n",
    "        return [\n",
    "            _process_frame(observation, self.conf)\n",
    "            for observation in observation_n\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalized environment class, where we can move from one state and observation to another"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NormalizedEnv(vectorized.ObservationWrapper):\n",
    "    def __init__(self, env=None):\n",
    "        super(NormalizedEnv, self).__init__(env)\n",
    "        self.state_mean = 0\n",
    "        self.state_std = 0\n",
    "        self.alpha = 0.9999\n",
    "        self.num_steps = 0\n",
    "\n",
    "    def _observation(self, observation_n):\n",
    "        for observation in observation_n:\n",
    "            self.num_steps += 1\n",
    "            self.state_mean = self.state_mean * self.alpha + \\\n",
    "                observation.mean() * (1 - self.alpha)\n",
    "            self.state_std = self.state_std * self.alpha + \\\n",
    "                observation.std() * (1 - self.alpha)\n",
    "\n",
    "        unbiased_mean = self.state_mean / (1 - pow(self.alpha, self.num_steps))\n",
    "        unbiased_std = self.state_std / (1 - pow(self.alpha, self.num_steps))\n",
    "\n",
    "        return [(observation - unbiased_mean) / (unbiased_std + 1e-8)\n",
    "                for observation in observation_n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class A3Clstm(torch.nn.Module):\n",
    "    def __init__(self, num_inputs, action_space):\n",
    "        super(A3Clstm, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(num_inputs, 32, 5, stride=1, padding=2)\n",
    "        self.maxp1 = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(32, 32, 5, stride=1, padding=1)\n",
    "        self.maxp2 = nn.MaxPool2d(2, 2)\n",
    "        self.conv3 = nn.Conv2d(32, 64, 4, stride=1, padding=1)\n",
    "        self.maxp3 = nn.MaxPool2d(2, 2)\n",
    "        self.conv4 = nn.Conv2d(64, 64, 3, stride=1, padding=1)\n",
    "        self.maxp4 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        self.lstm = nn.LSTMCell(1024, 512)\n",
    "        num_outputs = action_space.n\n",
    "        self.critic_linear = nn.Linear(512, 1)\n",
    "        self.actor_linear = nn.Linear(512, num_outputs)\n",
    "\n",
    "        self.apply(weights_init)\n",
    "        self.actor_linear.weight.data = norm_col_init(\n",
    "            self.actor_linear.weight.data, 0.01)\n",
    "        self.actor_linear.bias.data.fill_(0)\n",
    "        self.critic_linear.weight.data = norm_col_init(\n",
    "            self.critic_linear.weight.data, 1.0)\n",
    "        self.critic_linear.bias.data.fill_(0)\n",
    "\n",
    "        self.lstm.bias_ih.data.fill_(0)\n",
    "        self.lstm.bias_hh.data.fill_(0)\n",
    "\n",
    "        self.train()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        inputs, (hx, cx) = inputs\n",
    "        x = F.relu(self.maxp1(self.conv1(inputs)))\n",
    "        x = F.relu(self.maxp2(self.conv2(x)))\n",
    "        x = F.relu(self.maxp3(self.conv3(x)))\n",
    "        x = F.relu(self.maxp4(self.conv4(x)))\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        hx, cx = self.lstm(x, (hx, cx))\n",
    "\n",
    "        x = hx\n",
    "\n",
    "        return self.critic_linear(x), self.actor_linear(x), (hx, cx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The player Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    def __init__(self, model, env, args, state):\n",
    "        self.model = model\n",
    "        self.env = env\n",
    "        self.current_life = 0\n",
    "        self.state = state\n",
    "        self.hx = None\n",
    "        self.cx = None\n",
    "        self.eps_len = 0\n",
    "        self.args = args\n",
    "        self.values = []\n",
    "        self.log_probs = []\n",
    "        self.rewards = []\n",
    "        self.entropies = []\n",
    "        self.done = True\n",
    "        self.info = None\n",
    "        self.reward = 0\n",
    "\n",
    "    def action_train(self):\n",
    "        if self.done:\n",
    "            self.cx = Variable(torch.zeros(1, 512))\n",
    "            self.hx = Variable(torch.zeros(1, 512))\n",
    "        else:\n",
    "            self.cx = Variable(self.cx.data)\n",
    "            self.hx = Variable(self.hx.data)\n",
    "        value, logit, (self.hx, self.cx) = self.model((Variable(self.state.unsqueeze(0)), (self.hx, self.cx)))\n",
    "        prob = F.softmax(logit)\n",
    "        log_prob = F.log_softmax(logit)\n",
    "        entropy = -(log_prob * prob).sum(1)\n",
    "        self.entropies.append(entropy)\n",
    "        action = prob.multinomial().data\n",
    "        log_prob = log_prob.gather(1, Variable(action))\n",
    "        state, self.reward, self.done, self.info = self.env.step(action.numpy())\n",
    "        self.state = torch.from_numpy(state).float()\n",
    "        self.eps_len += 1\n",
    "        self.done = self.done or self.eps_len >= self.args['M']\n",
    "        self.reward = max(min(self.reward, 1), -1)\n",
    "        self.values.append(value)\n",
    "        self.log_probs.append(log_prob)\n",
    "        self.rewards.append(self.reward)\n",
    "        return self\n",
    "\n",
    "    def action_test(self):\n",
    "        if self.done:\n",
    "            self.cx = Variable(torch.zeros(1, 512), volatile=True)\n",
    "            self.hx = Variable(torch.zeros(1, 512), volatile=True)\n",
    "        else:\n",
    "            self.cx = Variable(self.cx.data, volatile=True)\n",
    "            self.hx = Variable(self.hx.data, volatile=True)\n",
    "        value, logit, (self.hx, self.cx) = self.model((Variable(self.state.unsqueeze(0), volatile=True), (self.hx, self.cx)))\n",
    "        prob = F.softmax(logit)\n",
    "        action = prob.max(1)[1].data.numpy()\n",
    "        state, self.reward, self.done, self.info = self.env.step(action[0])\n",
    "        self.state = torch.from_numpy(state).float()\n",
    "        self.eps_len += 1\n",
    "        self.done = self.done or self.eps_len >= self.args['M']\n",
    "        return self\n",
    "\n",
    "    def check_state(self):\n",
    "        if self.current_life > self.info['ale.lives']:\n",
    "            self.done = True\n",
    "        self.current_life = self.info['ale.lives']\n",
    "        return self\n",
    "\n",
    "    def clear_actions(self):\n",
    "        self.values = []\n",
    "        self.log_probs = []\n",
    "        self.rewards = []\n",
    "        self.entropies = []\n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shared Memory and optimization algorithims"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSprop\n",
    "\n",
    "RMSprop is an unpublished, adaptive learning rate method proposed by Geoff Hinton in Lecture 6e of his Coursera Class.\n",
    "\n",
    "RMSprop and Adadelta have both been developed independently around the same time stemming from the need to resolve Adagrad's radically diminishing learning rates. RMSprop in fact is identical to the first update vector of Adadelta \n",
    "\n",
    "RMSprop as well divides the learning rate by an exponentially decaying average of squared gradients. Hinton suggests γ\n",
    "to be set to 0.9, while a good default value for the learning rate η is 0.001."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SharedRMSprop(optim.RMSprop):\n",
    "    \"\"\"Implements RMSprop algorithm with shared states.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 params,\n",
    "                 lr=7e-4,\n",
    "                 alpha=0.99,\n",
    "                 eps=0.1,\n",
    "                 weight_decay=0,\n",
    "                 momentum=0,\n",
    "                 centered=False):\n",
    "        super(SharedRMSprop, self).__init__(params, lr, alpha, eps,\n",
    "                                            weight_decay, momentum, centered)\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                state = self.state[p]\n",
    "                state['step'] = torch.zeros(1)\n",
    "                state['grad_avg'] = p.data.new().resize_as_(p.data).zero_()\n",
    "                state['square_avg'] = p.data.new().resize_as_(p.data).zero_()\n",
    "                state['momentum_buffer'] = p.data.new().resize_as_(\n",
    "                    p.data).zero_()\n",
    "\n",
    "    def share_memory(self):\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                state = self.state[p]\n",
    "                state['square_avg'].share_memory_()\n",
    "                state['step'].share_memory_()\n",
    "                state['grad_avg'].share_memory_()\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"Performs a single optimization step.\n",
    "        Arguments:\n",
    "            closure (callable, optional): A closure that reevaluates the model\n",
    "                and returns the loss.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data\n",
    "                state = self.state[p]\n",
    "\n",
    "                square_avg = state['square_avg']\n",
    "                alpha = group['alpha']\n",
    "\n",
    "                state['step'] += 1\n",
    "\n",
    "                if group['weight_decay'] != 0:\n",
    "                    grad = grad.add(group['weight_decay'], p.data)\n",
    "\n",
    "                square_avg.mul_(alpha).addcmul_(1 - alpha, grad, grad)\n",
    "\n",
    "                if group['centered']:\n",
    "                    grad_avg = state['grad_avg']\n",
    "                    grad_avg.mul_(alpha).add_(1 - alpha, grad)\n",
    "                    avg = square_avg.addcmul(\n",
    "                        -1, grad_avg, grad_avg).sqrt().add_(group['eps'])\n",
    "                else:\n",
    "                    avg = square_avg.sqrt().add_(group['eps'])\n",
    "\n",
    "                if group['momentum'] > 0:\n",
    "                    buf = state['momentum_buffer']\n",
    "                    buf.mul_(group['momentum']).addcdiv_(grad, avg)\n",
    "                    p.data.add_(-group['lr'], buf)\n",
    "                else:\n",
    "                    p.data.addcdiv_(-group['lr'], grad, avg)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaptive Moment Estimation (Adam) [15] \n",
    "is another method that computes adaptive learning rates for each parameter. In addition to storing an exponentially decaying average of past squared gradients vt like Adadelta and RMSprop, Adam also keeps an exponentially decaying average of past gradients mt, similar to momentum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SharedAdam(optim.Adam):\n",
    "    \"\"\"Implements Adam algorithm with shared states.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 params,\n",
    "                 lr=1e-3,\n",
    "                 betas=(0.9, 0.999),\n",
    "                 eps=1e-3,\n",
    "                 weight_decay=0):\n",
    "        super(SharedAdam, self).__init__(params, lr, betas, eps, weight_decay)\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                state = self.state[p]\n",
    "                state['step'] = torch.zeros(1)\n",
    "                state['exp_avg'] = p.data.new().resize_as_(p.data).zero_()\n",
    "                state['exp_avg_sq'] = p.data.new().resize_as_(p.data).zero_()\n",
    "\n",
    "    def share_memory(self):\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                state = self.state[p]\n",
    "                state['step'].share_memory_()\n",
    "                state['exp_avg'].share_memory_()\n",
    "                state['exp_avg_sq'].share_memory_()\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"Performs a single optimization step.\n",
    "        Arguments:\n",
    "            closure (callable, optional): A closure that reevaluates the model\n",
    "                and returns the loss.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data\n",
    "                state = self.state[p]\n",
    "\n",
    "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "                beta1, beta2 = group['betas']\n",
    "\n",
    "                state['step'] += 1\n",
    "\n",
    "                if group['weight_decay'] != 0:\n",
    "                    grad = grad.add(group['weight_decay'], p.data)\n",
    "\n",
    "                # Decay the first and second moment running average coefficient\n",
    "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
    "\n",
    "                denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
    "\n",
    "                bias_correction1 = 1 - beta1**state['step'][0]\n",
    "                bias_correction2 = 1 - beta2**state['step'][0]\n",
    "                step_size = group['lr'] * \\\n",
    "                    math.sqrt(bias_correction2) / bias_correction1\n",
    "\n",
    "                p.data.addcdiv_(-step_size, exp_avg, denom)\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "sample_lr = [\n",
    "    0.0001, 0.00009, 0.00008, 0.00007, 0.00006, 0.00005, 0.00004, 0.00003,\n",
    "    0.00002, 0.00001, 0.000009, 0.000008, 0.000007, 0.000006, 0.000005,\n",
    "    0.000004, 0.000003, 0.000002, 0.000001\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam but with shared Lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SharedLrSchedAdam(optim.Adam):\n",
    "    \"\"\"Implements Adam algorithm with shared states.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 params,\n",
    "                 lr=1e-3,\n",
    "                 betas=(0.9, 0.999),\n",
    "                 eps=1e-3,\n",
    "                 weight_decay=0):\n",
    "        super(SharedLrSchedAdam, self).__init__(params, lr, betas, eps,\n",
    "                                                weight_decay)\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                state = self.state[p]\n",
    "                state['step'] = torch.zeros(1)\n",
    "                state['exp_avg'] = p.data.new().resize_as_(p.data).zero_()\n",
    "                state['exp_avg_sq'] = p.data.new().resize_as_(p.data).zero_()\n",
    "\n",
    "    def share_memory(self):\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                state = self.state[p]\n",
    "                state['step'].share_memory_()\n",
    "                state['exp_avg'].share_memory_()\n",
    "                state['exp_avg_sq'].share_memory_()\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"Performs a single optimization step.\n",
    "        Arguments:\n",
    "            closure (callable, optional): A closure that reevaluates the model\n",
    "                and returns the loss.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        lr = sample_lr[int(state['step'][0] // 40000000)]\n",
    "        group['lr'] = lr\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data\n",
    "                state = self.state[p]\n",
    "\n",
    "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "                beta1, beta2 = group['betas']\n",
    "\n",
    "                state['step'] += 1\n",
    "\n",
    "                if group['weight_decay'] != 0:\n",
    "                    grad = grad.add(group['weight_decay'], p.data)\n",
    "\n",
    "                # Decay the first and second moment running average coefficient\n",
    "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
    "\n",
    "                denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
    "\n",
    "                bias_correction1 = 1 - beta1**state['step'][0]\n",
    "                bias_correction2 = 1 - beta2**state['step'][0]\n",
    "                step_size = group['lr'] * \\\n",
    "                    math.sqrt(bias_correction2) / bias_correction1\n",
    "                p.data.addcdiv_(-step_size, exp_avg, denom)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test\n",
    "## Function To test the model on a game/environ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test(args, shared_model, env_conf):\n",
    "    log = {}\n",
    "    setup_logger('{}_log'.format(args['ENV']),\n",
    "                 r'{0}{1}_log'.format(args['LG'], args['ENV']))\n",
    "    log['{}_log'.format(args['ENV'])] = logging.getLogger(\n",
    "        '{}_log'.format(args['ENV']))\n",
    "    d_args = args\n",
    "    for k in d_args.keys():\n",
    "        log['{}_log'.format(args['ENV'])].info('{0}: {1}'.format(k, d_args[k]))\n",
    "\n",
    "    torch.manual_seed(args['seed'])\n",
    "    env = atari_env(args['ENV'], env_conf)\n",
    "    reward_sum = 0\n",
    "    start_time = time.time()\n",
    "    num_tests = 0\n",
    "    reward_total_sum = 0\n",
    "    player = Agent(None, env, args, None)\n",
    "    player.model = A3Clstm(\n",
    "        player.env.observation_space.shape[0], player.env.action_space)\n",
    "    player.state = player.env.reset()\n",
    "    player.state = torch.from_numpy(player.state).float()\n",
    "    player.model.eval()\n",
    "\n",
    "    while True:\n",
    "        if player.done:\n",
    "            player.model.load_state_dict(shared_model.state_dict())\n",
    "\n",
    "        player.action_test()\n",
    "        reward_sum += player.reward\n",
    "\n",
    "        if player.done:\n",
    "            num_tests += 1\n",
    "            player.current_life = 0\n",
    "            reward_total_sum += reward_sum\n",
    "            reward_mean = reward_total_sum / num_tests\n",
    "            log['{}_log'.format(args['ENV'])].info(\n",
    "                \"Time {0}, episode reward {1}, episode length {2}, reward mean {3:.4f}\".\n",
    "                format(\n",
    "                    time.strftime(\"%Hh %Mm %Ss\",\n",
    "                                  time.gmtime(time.time() - start_time)),\n",
    "                    reward_sum, player.eps_len, reward_mean))\n",
    "\n",
    "            if reward_sum > args['SSL']:\n",
    "                player.model.load_state_dict(shared_model.state_dict())\n",
    "                state_to_save = player.model.state_dict()\n",
    "                torch.save(state_to_save, '{0}{1}.dat'.format(\n",
    "                    args['SMD'], args['ENV']))\n",
    "\n",
    "            reward_sum = 0\n",
    "            player.eps_len = 0\n",
    "            state = player.env.reset()\n",
    "            time.sleep(60)\n",
    "            player.state = torch.from_numpy(state).float()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train\n",
    "## Function to Train the model with an optimizer algorithim on an environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(rank, args, shared_model, optimizer, env_conf):\n",
    "\n",
    "    torch.manual_seed(args['seed'] + rank)\n",
    "    env = atari_env(args['ENV'], env_conf)\n",
    "    if optimizer is None:\n",
    "        if args['OPT'] == 'RMSprop':\n",
    "            optimizer = optim.RMSprop(shared_model.parameters(), lr=args['LR'])\n",
    "        if args['OPT'] == 'Adam':\n",
    "            optimizer = optim.Adam(shared_model.parameters(), lr=args['LR'])\n",
    "\n",
    "    env.seed(args['seed'] + rank)\n",
    "    player = Agent(None, env, args, None)\n",
    "    player.model = A3Clstm(\n",
    "        player.env.observation_space.shape[0], player.env.action_space)\n",
    "    player.state = player.env.reset()\n",
    "    player.state = torch.from_numpy(player.state).float()\n",
    "    player.model.train()\n",
    "\n",
    "    while True:\n",
    "        player.model.load_state_dict(shared_model.state_dict())\n",
    "        for step in range(args['NS']):\n",
    "            player.action_train()\n",
    "            if args['CL']:\n",
    "                player.check_state()\n",
    "            if player.done:\n",
    "                break\n",
    "\n",
    "        if player.done:\n",
    "            player.eps_len = 0\n",
    "            player.current_life = 0\n",
    "            state = player.env.reset()\n",
    "            player.state = torch.from_numpy(state).float()\n",
    "\n",
    "        R = torch.zeros(1, 1)\n",
    "        if not player.done:\n",
    "            value, _, _ = player.model(\n",
    "                (Variable(player.state.unsqueeze(0)), (player.hx, player.cx)))\n",
    "            R = value.data\n",
    "\n",
    "        player.values.append(Variable(R))\n",
    "        policy_loss = 0\n",
    "        value_loss = 0\n",
    "        R = Variable(R)\n",
    "        gae = torch.zeros(1, 1)\n",
    "        for i in reversed(range(len(player.rewards))):\n",
    "            R = args['G'] * R + player.rewards[i]\n",
    "            advantage = R - player.values[i]\n",
    "            value_loss = value_loss + 0.5 * advantage.pow(2)\n",
    "\n",
    "            # Generalized Advantage Estimataion\n",
    "            delta_t = player.rewards[i] + args['G'] * \\\n",
    "                player.values[i + 1].data - player.values[i].data\n",
    "            gae = gae * args['G'] * args['T'] + delta_t\n",
    "\n",
    "            policy_loss = policy_loss - \\\n",
    "                player.log_probs[i] * \\\n",
    "                Variable(gae) - 0.01 * player.entropies[i]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        (policy_loss + 0.5 * value_loss).backward()\n",
    "        torch.nn.utils.clip_grad_norm(player.model.parameters(), 40)\n",
    "        ensure_shared_grads(player.model, shared_model)\n",
    "        optimizer.step()\n",
    "        player.clear_actions()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting it altogether"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "undo_logger_setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    type=float,\n",
    "    metavar='LR',\n",
    "    help='learning rate (default: 0.0001)')\n",
    "\n",
    "    type=float,\n",
    "    metavar='G',\n",
    "    help='discount factor for rewards (default: 0.99)')\n",
    "\n",
    "    type=float,\n",
    "    metavar='T',\n",
    "    help='parameter for GAE (default: 1.00)')\n",
    "\n",
    "    type=int,\n",
    "    metavar='S',\n",
    "    help='random seed (default: 1)')\n",
    "\n",
    "    type=int,\n",
    "    metavar='W',\n",
    "    help='how many training processes to use (default: 32)')\n",
    "\n",
    "    type=int,\n",
    "    metavar='NS',\n",
    "    help='number of forward steps in A3C (default: 20)')\n",
    "\n",
    "    type=int,\n",
    "    metavar='M',\n",
    "    help='maximum length of an episode (default: 10000)')\n",
    "\n",
    "    metavar='ENV',\n",
    "    help='environment to train on (default: Pong-v0)')\n",
    "\n",
    "    metavar='EC',\n",
    "    help='environment to crop and resize info (default: config.json)')\n",
    "\n",
    "    default=True,\n",
    "    metavar='SO',\n",
    "    help='use an optimizer without shared statistics.')\n",
    "\n",
    "    default=False,\n",
    "    metavar='L',\n",
    "    help='load a trained model')\n",
    "\n",
    "    type=int,\n",
    "    default=20,\n",
    "    metavar='SSL',\n",
    "    help='reward score test evaluation must get higher than to save model')\n",
    "\n",
    "    default='Adam',\n",
    "    metavar='OPT',\n",
    "    help='shares optimizer choice of Adam or RMSprop')\n",
    "\n",
    "    default=False,\n",
    "    metavar='CL',\n",
    "    help='end of life is end of training episode.')\n",
    "\n",
    "    default='trained_models/',\n",
    "    metavar='LMD',\n",
    "    help='folder to load trained models from')\n",
    "\n",
    "    default='trained_models/',\n",
    "    metavar='SMD',\n",
    "    help='folder to save trained models')\n",
    "\n",
    "    default='logs/',\n",
    "    metavar='LG',\n",
    "    help='folder to save logs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "args = {'LR': 0.0001, \"G\":0.99, \"T\":1.00, \"S\":1,\"W\":32,\"NS\":20,\"M\":10000,\"ENV\":'Pong-v0',\n",
    "         \"EC\":'config.json',\"SO\":True,\"L\":True,\"SSL\":20, \"OPT\":\"Adam\",\"CL\":False,\n",
    "         \"LMD\":'trained_models/',\"SMD\":\"trained_models/\",\"LG\":'logs/', \"seed\":42\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running an Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch.set_default_tensor_type('torch.FloatTensor')\n",
    "torch.manual_seed(args['seed'])\n",
    "\n",
    "setup_json = read_config(args['EC'])\n",
    "\n",
    "env_conf = setup_json[\"Default\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in setup_json.keys():\n",
    "    if i in args['ENV']:\n",
    "        env_conf = setup_json[i]\n",
    "env = atari_env(args['ENV'], env_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "A3Clstm (\n",
       "  (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (maxp1): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
       "  (conv2): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
       "  (maxp2): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
       "  (conv3): Conv2d(32, 64, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
       "  (maxp3): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
       "  (conv4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (maxp4): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
       "  (lstm): LSTMCell(1024, 512)\n",
       "  (critic_linear): Linear (512 -> 1)\n",
       "  (actor_linear): Linear (512 -> 6)\n",
       ")"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shared_model = A3Clstm(env.observation_space.shape[0], env.action_space)\n",
    "if args['L']:\n",
    "    saved_state = torch.load(\n",
    "        '{0}{1}.dat'.format(args['LMD'], args['ENV']))\n",
    "    shared_model.load_state_dict(saved_state)\n",
    "shared_model.share_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if args['SO']:\n",
    "    if args['OPT'] == 'RMSprop':\n",
    "        optimizer = SharedRMSprop(shared_model.parameters(), lr=args['LR'])\n",
    "    if args['OPT'] == 'Adam':\n",
    "        optimizer = SharedAdam(shared_model.parameters(), lr=args['LR'])\n",
    "    if args['OPT'] == 'LrSchedAdam':\n",
    "        optimizer = SharedLrSchedAdam(\n",
    "            shared_model.parameters(), lr=args['LR'])\n",
    "    optimizer.share_memory()\n",
    "else:\n",
    "    optimizer = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run This to Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-10-06 15:06:35,283 : OPT: Adam\n",
      "2017-10-06 15:06:35,285 : LG: logs/\n",
      "2017-10-06 15:06:35,286 : SMD: trained_models/\n",
      "2017-10-06 15:06:35,288 : ENV: Pong-v0\n",
      "2017-10-06 15:06:35,289 : G: 0.99\n",
      "2017-10-06 15:06:35,290 : CL: False\n",
      "2017-10-06 15:06:35,291 : M: 10000\n",
      "2017-10-06 15:06:35,291 : L: True\n",
      "2017-10-06 15:06:35,292 : EC: config.json\n",
      "2017-10-06 15:06:35,293 : SSL: 20\n",
      "2017-10-06 15:06:35,294 : S: 1\n",
      "2017-10-06 15:06:35,295 : seed: 42\n",
      "2017-10-06 15:06:35,295 : LR: 0.0001\n",
      "2017-10-06 15:06:35,296 : T: 1.0\n",
      "2017-10-06 15:06:35,297 : W: 32\n",
      "2017-10-06 15:06:35,297 : SO: True\n",
      "2017-10-06 15:06:35,298 : NS: 20\n",
      "2017-10-06 15:06:35,299 : LMD: trained_models/\n",
      "2017-10-06 15:07:04,665 : Time 00h 00m 29s, episode reward -21.0, episode length 1009, reward mean -21.0000\n",
      "2017-10-06 15:08:12,870 : Time 00h 01m 37s, episode reward -21.0, episode length 1027, reward mean -21.0000\n",
      "2017-10-06 15:09:21,614 : Time 00h 02m 46s, episode reward -21.0, episode length 1029, reward mean -21.0000\n",
      "2017-10-06 15:10:35,920 : Time 00h 04m 00s, episode reward -21.0, episode length 1027, reward mean -21.0000\n",
      "2017-10-06 15:11:48,500 : Time 00h 05m 13s, episode reward -21.0, episode length 1020, reward mean -21.0000\n",
      "2017-10-06 15:12:56,741 : Time 00h 06m 21s, episode reward -21.0, episode length 1017, reward mean -21.0000\n",
      "2017-10-06 15:14:04,530 : Time 00h 07m 29s, episode reward -21.0, episode length 1009, reward mean -21.0000\n",
      "2017-10-06 15:15:12,522 : Time 00h 08m 37s, episode reward -21.0, episode length 1027, reward mean -21.0000\n",
      "2017-10-06 15:16:20,276 : Time 00h 09m 44s, episode reward -21.0, episode length 1028, reward mean -21.0000\n",
      "2017-10-06 15:17:28,366 : Time 00h 10m 52s, episode reward -21.0, episode length 1026, reward mean -21.0000\n",
      "2017-10-06 15:18:39,684 : Time 00h 12m 04s, episode reward -21.0, episode length 1019, reward mean -21.0000\n",
      "2017-10-06 15:19:49,379 : Time 00h 13m 13s, episode reward -21.0, episode length 1004, reward mean -21.0000\n",
      "2017-10-06 15:20:58,213 : Time 00h 14m 22s, episode reward -21.0, episode length 1007, reward mean -21.0000\n"
     ]
    }
   ],
   "source": [
    "processes = []\n",
    "\n",
    "p = Process(target=test, args=(args, shared_model, env_conf))\n",
    "p.start()\n",
    "processes.append(p)\n",
    "time.sleep(0.1)\n",
    "for rank in range(0, args['W']):\n",
    "    p = Process(\n",
    "        target=train, args=(rank, args, shared_model, optimizer, env_conf))\n",
    "    p.start()\n",
    "    processes.append(p)\n",
    "    time.sleep(0.1)\n",
    "for p in processes:\n",
    "    time.sleep(0.1)\n",
    "    p.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playing the Atari Games\n",
    "### The best part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
